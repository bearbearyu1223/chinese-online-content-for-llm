---
layout: post
title: "Study Notes: Stanford CS336 Language Modeling from Scratch [2]"
categories: cs336
author:
- å¤§æ¨¡å‹æˆ‘éƒ½çˆ±
---

<style>
  .xiaohongshu-link {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    color: #ff2442; /* å°çº¢ä¹¦ä¸»è‰² */
    text-decoration: none;
    font-weight: bold;
    font-size: 14px;
  }
  .xiaohongshu-link:hover {
    text-decoration: underline;
  }
  .xiaohongshu-logo {
    width: 18px;
    height: 18px;
    border-radius: 4px;
  }
</style>

<div style="padding:12px;border:1px solid #eee;border-radius:8px;display:inline-block;margin-bottom:20px;">
  <strong>å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</strong><br>
  <p style="margin:4px 0;">
    å°çº¢ä¹¦å·ï¼š
    <a class="xiaohongshu-link"
       href="https://www.xiaohongshu.com/user/profile/5b2c5758e8ac2b08bf20e38d"
       target="_blank">
      <img class="xiaohongshu-logo"
           src="https://static.cdnlogo.com/logos/r/77/rednote-xiaohongshu.svg"
           alt="å°çº¢ä¹¦ logo">
      119826921
    </a>
  </p>
  IPå±åœ°ï¼šç¾å›½
</div>

# Byte Pair Encoding (BPE) Tokenizer in a Nutshell

# å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰åˆ†è¯å™¨ç®€è¿°

---

## Key Terms

## å…³é”®æœ¯è¯­

| Concept | Description |
|:--------|:------------|
| Unicode | System that assigns every character a unique codepoint (e.g., 'A' â†’ 65) |
| UTF-8 | A way to encode those codepoints into 1-4 bytes |
| Byte | 8 bits; one byte can hold values from 0 to 255 |
| Tokenization | Breaking text corpus input into manageable pieces (tokens) for a model |

| æ¦‚å¿µ | æè¿° |
|:--------|:------------|
| Unicode | ä¸ºæ¯ä¸ªå­—ç¬¦åˆ†é…å”¯ä¸€ä»£ç ç‚¹çš„ç³»ç»Ÿï¼ˆä¾‹å¦‚ï¼Œ'A' â†’ 65ï¼‰ |
| UTF-8 | å°†è¿™äº›ä»£ç ç‚¹ç¼–ç ä¸º1-4å­—èŠ‚çš„æ–¹æ³• |
| å­—èŠ‚ | 8ä½ï¼›ä¸€ä¸ªå­—èŠ‚å¯ä»¥ä¿å­˜0åˆ°255çš„å€¼ |
| åˆ†è¯ | å°†æ–‡æœ¬è¯­æ–™åº“è¾“å…¥åˆ†è§£ä¸ºæ¨¡å‹å¯ç®¡ç†çš„ç‰‡æ®µï¼ˆæ ‡è®°ï¼‰ |

---

Let us take the following string as a simple example to illustrate the concept.

è®©æˆ‘ä»¬ä»¥ä¸‹é¢çš„å­—ç¬¦ä¸²ä½œä¸ºä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜è¿™ä¸ªæ¦‚å¿µã€‚

---

## Example: Encoding 'AğŸ˜Š'

## ç¤ºä¾‹ï¼šç¼–ç 'AğŸ˜Š'

---

### Step 1: Get the Unicode Codepoints

```python
text = 'AğŸ˜Š'
codepoints = [ord('A'), ord('ğŸ˜Š')]
print(codepoints)  # [65, 128522]
```

Output:
```
[65, 128522]
```

### æ­¥éª¤1ï¼šè·å–Unicodeä»£ç 

```python
text = 'AğŸ˜Š'
codepoints = [ord('A'), ord('ğŸ˜Š')]
print(codepoints)  # [65, 128522]
```

è¾“å‡ºï¼š
```
[65, 128522]
```

---

### Step 2: UTF-8 Encoding (Turn Codepoints into Bytes)

```python
utf8_bytes = text.encode("utf-8")
print(tuple(utf8_bytes))  # (65, 240, 159, 152, 138)
```

Output:
```
(65, 240, 159, 152, 138)
```

Here's what happened:
- 'A' is encoded using one byte: 65
- 'ğŸ˜Š' is encoded using four bytes: [240, 159, 152, 138]
- 'AğŸ˜Š' is encoded as the sequence [65, 240, 159, 152, 138]

### æ­¥éª¤2ï¼šUTF-8ç¼–ç ï¼ˆå°†ä»£ç è½¬æ¢ä¸ºå­—èŠ‚ï¼‰

```python
utf8_bytes = text.encode("utf-8")
print(tuple(utf8_bytes))  # (65, 240, 159, 152, 138)
```

è¾“å‡ºï¼š
```
(65, 240, 159, 152, 138)
```

å‘ç”Ÿäº†ä»€ä¹ˆï¼š
- 'A' ä½¿ç”¨ä¸€ä¸ªå­—èŠ‚ç¼–ç ï¼š65
- 'ğŸ˜Š' ä½¿ç”¨å››ä¸ªå­—èŠ‚ç¼–ç ï¼š[240, 159, 152, 138]
- 'AğŸ˜Š' è¢«ç¼–ç ä¸ºåºåˆ— [65, 240, 159, 152, 138]

---

## Why Using UTF-8 for Encoding is Helpful

Instead of dealing with hundreds of thousands of possible codepoints (Unicode has more than 150,000 codepoints) or millions of words/subwords in vocabulary, we can model text using sequences of bytes. Each byte can be represented by an integer from 0 to 255, so we only need a vocabulary of size 256 to model input text. This approach is simple and completeâ€”any character in any language can be represented as bytes, eliminating out-of-vocabulary token concerns.

## ä¸ºä»€ä¹ˆä½¿ç”¨UTF-8ç¼–ç å¾ˆæœ‰å¸®åŠ©

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å­—èŠ‚åºåˆ—æ¥å»ºæ¨¡æ–‡æœ¬ï¼Œè€Œä¸æ˜¯å¤„ç†æ•°åä¸‡ä¸ªå¯èƒ½çš„ä»£ç ç‚¹ï¼ˆUnicodeæœ‰è¶…è¿‡150,000ä¸ªä»£ç ç‚¹ï¼‰æˆ–è¯æ±‡è¡¨ä¸­çš„æ•°ç™¾ä¸‡ä¸ªå•è¯/å­è¯ã€‚æ¯ä¸ªå­—èŠ‚å¯ä»¥ç”¨0åˆ°255ä¹‹é—´çš„æ•´æ•°è¡¨ç¤ºï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å¤§å°ä¸º256çš„è¯æ±‡è¡¨æ¥å»ºæ¨¡è¾“å…¥æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•ç®€å•ä¸”å®Œæ•´â€”â€”ä»»ä½•è¯­è¨€ä¸­çš„ä»»ä½•å­—ç¬¦éƒ½å¯ä»¥è¡¨ç¤ºä¸ºå­—èŠ‚ï¼Œæ¶ˆé™¤äº†è¯æ±‡è¡¨å¤–æ ‡è®°çš„é—®é¢˜ã€‚

---

## Tokenization Spectrum

## åˆ†è¯èŒƒå›´

| Tokenization Level | Example Tokens | Pros | Cons |
|:-------------------|:---------------|:-----|:-----|
| **Word-level** | `["unbelievable"]` | Human-readable, efficient | OOV (out-of-vocabulary) issues |
| **Subword-level** (BPE) | `["un", "believ", "able"]` | Handles rare words, compact | Requires training |
| **Byte-level** | `[117, 110, 98, 101, ...]` (bytes) | No OOV, simple | Longer sequences, less semantic meaning |

| åˆ†è¯çº§åˆ« | ç¤ºä¾‹æ ‡è®° | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|:-------------------|:---------------|:-----|:-----|
| **è¯çº§** | `["unbelievable"]` | äººç±»å¯è¯»ï¼Œé«˜æ•ˆ | OOVï¼ˆè¯æ±‡è¡¨å¤–ï¼‰é—®é¢˜ |
| **å­è¯çº§** (BPE) | `["un", "believ", "able"]` | å¤„ç†ç½•è§è¯ï¼Œç´§å‡‘ | éœ€è¦è®­ç»ƒ |
| **å­—èŠ‚çº§** | `[117, 110, 98, 101, ...]` (å­—èŠ‚) | æ— OOVï¼Œç®€å• | åºåˆ—æ›´é•¿ï¼Œè¯­ä¹‰æ„ä¹‰è¾ƒå°‘ |

---

## Why Subword Tokenization is the Middle Ground

**Subword tokenization** with **Byte Pair Encoding (BPE)** provides a balance between the other approaches:

- **Word-level tokenization** struggles with rare or unseen words (e.g., "unbelievable" might be unknown even if "believe" is known)
- **Byte-level tokenization** avoids unknown token issues but creates long, inefficient sequences
- **Subword tokenization** (BPE):
  1. Breaks rare words into familiar pieces (subwords)
  2. Retains compactness for common words
  3. Is learnable from corpus statistics

## ä¸ºä»€ä¹ˆå­è¯åˆ†è¯æ˜¯ä¸­é—´åœ°å¸¦

ä½¿ç”¨**å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰**çš„**å­è¯åˆ†è¯**åœ¨å…¶ä»–æ–¹æ³•ä¹‹é—´æä¾›äº†å¹³è¡¡ï¼š

- **è¯çº§åˆ†è¯**åœ¨å¤„ç†ç½•è§æˆ–æœªè§è¿‡çš„è¯æ—¶é‡åˆ°å›°éš¾ï¼ˆä¾‹å¦‚ï¼Œå³ä½¿"believe"æ˜¯å·²çŸ¥çš„ï¼Œ"unbelievable"å¯èƒ½æ˜¯æœªçŸ¥çš„ï¼‰
- **å­—èŠ‚çº§åˆ†è¯**é¿å…äº†æœªçŸ¥æ ‡è®°é—®é¢˜ï¼Œä½†åˆ›å»ºäº†å†—é•¿ã€ä½æ•ˆçš„åºåˆ—
- **å­è¯åˆ†è¯**ï¼ˆBPEï¼‰ï¼š
  1. å°†ç½•è§è¯åˆ†è§£ä¸ºç†Ÿæ‚‰çš„ç‰‡æ®µï¼ˆå­è¯ï¼‰
  2. ä¿æŒå¸¸è§è¯çš„ç´§å‡‘æ€§
  3. å¯ä»¥ä»è¯­æ–™åº“ç»Ÿè®¡ä¸­å­¦ä¹ 

---

## Byte Pair Encoding (BPE) Algorithm Overview

BPE starts from characters and iteratively **merges the most frequent adjacent pairs** into longer tokens.

## å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•æ¦‚è¿°

BPEä»å­—ç¬¦å¼€å§‹ï¼Œè¿­ä»£åœ°**å°†æœ€é¢‘ç¹çš„ç›¸é‚»å¯¹åˆå¹¶**ä¸ºæ›´é•¿çš„æ ‡è®°ã€‚

---

### Example Training Corpus

Consider this toy training corpus:

```
"low"     (5 times)
"lowest"  (2 times)
"newest"  (6 times)
"wider"   (3 times)
```

We want to learn a compact subword vocabulary that reuses frequent patterns like "low" and "est".

### ç¤ºä¾‹è®­ç»ƒè¯­æ–™åº“

è€ƒè™‘è¿™ä¸ªç©å…·è®­ç»ƒè¯­æ–™åº“ï¼š

```
"low"     (5æ¬¡)
"lowest"  (2æ¬¡)
"newest"  (6æ¬¡)
"wider"   (3æ¬¡)
```

æˆ‘ä»¬æƒ³è¦å­¦ä¹ ä¸€ä¸ªç´§å‡‘çš„å­è¯è¯æ±‡è¡¨ï¼Œé‡ç”¨åƒ"low"å’Œ"est"è¿™æ ·çš„é¢‘ç¹æ¨¡å¼ã€‚

---

### Step-by-Step BPE Process

### é€æ­¥BPEè¿‡ç¨‹

---

#### Step 0: Preprocess as Characters

Each word is broken into characters with an end-of-word marker `</w>`:

```
"l o w </w>"        (5)
"l o w e s t </w>"  (2)
"n e w e s t </w>"  (6)
"w i d e r </w>"    (3)
```

#### æ­¥éª¤0ï¼šé¢„å¤„ç†ä¸ºå­—ç¬¦

æ¯ä¸ªå•è¯è¢«åˆ†è§£ä¸ºå­—ç¬¦ï¼Œå¸¦æœ‰è¯å°¾æ ‡è®°`</w>`ï¼š

```
"l o w </w>"        (5)
"l o w e s t </w>"  (2)
"n e w e s t </w>"  (6)
"w i d e r </w>"    (3)
```

---

#### Step 1: Count Adjacent Pairs

Compute most frequent adjacent pairs across all words:

```
('e', 's') appears 8 times
('s', 't') appears 8 times
('l', 'o') appears 7 times
('o', 'w') appears 7 times
```

#### æ­¥éª¤1ï¼šè®¡æ•°ç›¸é‚»å¯¹

è®¡ç®—æ‰€æœ‰å•è¯ä¸­æœ€é¢‘ç¹çš„ç›¸é‚»å¯¹ï¼š

```
('e', 's') å‡ºç°8æ¬¡
('s', 't') å‡ºç°8æ¬¡
('l', 'o') å‡ºç°7æ¬¡
('o', 'w') å‡ºç°7æ¬¡
```

---

#### Step 2: Merge 'e' + 's' â†’ 'es'

Update vocabulary:

```
"l o w </w>"          (5)
"l o w es t </w>"     (2)
"n e w es t </w>"     (6)
"w i d e r </w>"      (3)
```

#### æ­¥éª¤2ï¼šåˆå¹¶ 'e' + 's' â†’ 'es'

æ›´æ–°è¯æ±‡è¡¨ï¼š

```
"l o w </w>"          (5)
"l o w es t </w>"     (2)
"n e w es t </w>"     (6)
"w i d e r </w>"      (3)
```

---

#### Step 3: Merge 'es' + 't' â†’ 'est'

```
"l o w </w>"         (5)
"l o w est </w>"     (2)
"n e w est </w>"     (6)
"w i d e r </w>"     (3)
```

#### æ­¥éª¤3ï¼šåˆå¹¶ 'es' + 't' â†’ 'est'

```
"l o w </w>"         (5)
"l o w est </w>"     (2)
"n e w est </w>"     (6)
"w i d e r </w>"     (3)
```

---

#### Step 4: Merge 'l' + 'o' â†’ 'lo', then 'lo' + 'w' â†’ 'low'

```
"low </w>"          (5)
"low est </w>"      (2)
"n e w est </w>"    (6)
"w i d e r </w>"    (3)
```

#### æ­¥éª¤4ï¼šåˆå¹¶ 'l' + 'o' â†’ 'lo'ï¼Œç„¶å 'lo' + 'w' â†’ 'low'

```
"low </w>"          (5)
"low est </w>"      (2)
"n e w est </w>"    (6)
"w i d e r </w>"    (3)
```

---

#### Continue merging...

Eventually we learn useful building blocks like 'low', 'est', and 'new'. After training, "newest" would tokenize to `['new', 'est', '</w>']`.

#### ç»§ç»­åˆå¹¶...

æœ€ç»ˆæˆ‘ä»¬å­¦ä¹ åˆ°æœ‰ç”¨çš„æ„å»ºå—ï¼Œå¦‚'low'ã€'est'å’Œ'new'ã€‚è®­ç»ƒåï¼Œ"newest"å°†è¢«åˆ†è¯ä¸º`['new', 'est', '</w>']`ã€‚

---

## BPE Implementation

Below is a complete implementation demonstrating the BPE algorithm on the corpus:

```
"low low low low low lower lower widest widest widest newest newest newest newest newest newest"
```

## BPEå®ç°

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„å®ç°ï¼Œæ¼”ç¤ºäº†åœ¨è¯­æ–™åº“ä¸Šçš„BPEç®—æ³•ï¼š

```
"low low low low low lower lower widest widest widest newest newest newest newest newest newest"
```

---

### Key Components

1. **Initialization**: Creates vocabulary with `<|endoftext|>` special token and all 256 byte values
2. **Pre-tokenization**: Splits text on whitespace and converts words to byte tuples
3. **Pair Frequency Counting**: Counts all adjacent byte pairs across the corpus
4. **Merging**: Merges the most frequent pair (lexicographically largest in case of ties)
5. **Tokenization**: Uses learned merges to tokenize new words

### å…³é”®ç»„ä»¶

1. **åˆå§‹åŒ–**ï¼šåˆ›å»ºåŒ…å«`<|endoftext|>`ç‰¹æ®Šæ ‡è®°å’Œæ‰€æœ‰256ä¸ªå­—èŠ‚å€¼çš„è¯æ±‡è¡¨
2. **é¢„åˆ†è¯**ï¼šåœ¨ç©ºæ ¼å¤„åˆ†å‰²æ–‡æœ¬å¹¶å°†å•è¯è½¬æ¢ä¸ºå­—èŠ‚å…ƒç»„
3. **å¯¹é¢‘ç‡è®¡æ•°**ï¼šè®¡ç®—è¯­æ–™åº“ä¸­æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹
4. **åˆå¹¶**ï¼šåˆå¹¶æœ€é¢‘ç¹çš„å¯¹ï¼ˆåœ¨å¹³å±€çš„æƒ…å†µä¸‹é€‰æ‹©å­—å…¸åºæœ€å¤§çš„ï¼‰
5. **åˆ†è¯**ï¼šä½¿ç”¨å­¦ä¹ åˆ°çš„åˆå¹¶æ¥åˆ†è¯æ–°å•è¯

---

### How It Works

1. **Pre-tokenization**: Converts `"low low low..."` into `{(l,o,w): 5, (l,o,w,e,r): 2, ...}`
2. **Merge Selection**: Finds most frequent pairs like `('s','t')` and `('e','s')`, chooses lexicographically larger `('s','t')`
3. **Iterative Merging**: Continues merging until desired number of merges is reached
4. **Tokenization**: Applies learned merges in order to tokenize new words

### å·¥ä½œåŸç†

1. **é¢„åˆ†è¯**ï¼šå°†`"low low low..."`è½¬æ¢ä¸º`{(l,o,w): 5, (l,o,w,e,r): 2, ...}`
2. **åˆå¹¶é€‰æ‹©**ï¼šæ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹ï¼Œå¦‚`('s','t')`å’Œ`('e','s')`ï¼Œé€‰æ‹©å­—å…¸åºè¾ƒå¤§çš„`('s','t')`
3. **è¿­ä»£åˆå¹¶**ï¼šç»§ç»­åˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„åˆå¹¶æ¬¡æ•°
4. **åˆ†è¯**ï¼šæŒ‰é¡ºåºåº”ç”¨å­¦ä¹ åˆ°çš„åˆå¹¶æ¥åˆ†è¯æ–°å•è¯

---

### Expected Output

With 6 merges, the algorithm produces:
- **Merges**: `['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']`
- **Final vocabulary**: `<|endoftext|>`, 256 byte chars, `st`, `est`, `ow`, `low`, `west`, `ne`
- **"newest" tokenizes as**: `['ne', 'west']`

### é¢„æœŸè¾“å‡º

é€šè¿‡6æ¬¡åˆå¹¶ï¼Œç®—æ³•äº§ç”Ÿï¼š
- **åˆå¹¶**ï¼š`['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']`
- **æœ€ç»ˆè¯æ±‡è¡¨**ï¼š`<|endoftext|>`ã€256ä¸ªå­—èŠ‚å­—ç¬¦ã€`st`ã€`est`ã€`ow`ã€`low`ã€`west`ã€`ne`
- **"newest"è¢«åˆ†è¯ä¸º**ï¼š`['ne', 'west']`

---

Below is one implementation for Algorithm 1 of [Sennrich et al. [2016]](https://arxiv.org/abs/1508.07909).

ä¸‹é¢æ˜¯[Sennrichç­‰äºº[2016]](https://arxiv.org/abs/1508.07909)çš„ç®—æ³•1çš„ä¸€ä¸ªå®ç°ã€‚

```python
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set, Union

class BPEEncoder:
    def __init__(self):
        # Initialize vocabulary with special token and 256 byte values
        self.vocab = {"<|endoftext|>": 0}
        # Add all possible byte values (0-255) to vocabulary
        for i in range(256):
            self.vocab[i] = i + 1

        self.merges = []  # List of (token1, token2) pairs that were merged
        self.merge_tokens = {}  # Maps (token1, token2) -> new_token_id
        self.token_names = {}  # Maps token_id -> readable name
        self.next_token_id = 257

    def pre_tokenize(self, text: str) -> Dict[Tuple[int, ...], int]:
        """
        Pre-tokenize text by splitting on whitespace and convert to byte tuples.
        Returns frequency count of each word as tuple of byte integers.
        For example, converts "low low low..." into {(l,o,w): 5, (l,o,w,e,r): 2, ...}
        """
        words = text.split()
        word_freq = Counter(words)

        # Convert to tuple of byte integers
        byte_word_freq = {}
        for word, freq in word_freq.items():
            byte_tuple = tuple(word.encode('utf-8'))
            byte_word_freq[byte_tuple] = freq

        return byte_word_freq

    def get_pair_frequencies(self, word_freq: Dict[Tuple[Union[int, str], ...], int]) -> Dict[Tuple[Union[int, str], Union[int, str]], int]:
        """
        Count frequency of all adjacent token pairs across all words.
        """
        pair_freq = defaultdict(int)

        for word, freq in word_freq.items():
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                pair_freq[pair] += freq

        return dict(pair_freq)

    def merge_pair(self, word_freq: Dict[Tuple[Union[int, str], ...], int],
                   pair_to_merge: Tuple[Union[int, str], Union[int, str]],
                   new_token: str) -> Dict[Tuple[Union[int, str], ...], int]:
        """
        Merge the specified pair in all words where it appears.
        """
        new_word_freq = {}

        for word, freq in word_freq.items():
            new_word = []
            i = 0
            while i < len(word):
                # Check if current position matches the pair to merge
                if (i < len(word) - 1 and
                    word[i] == pair_to_merge[0] and
                    word[i + 1] == pair_to_merge[1]):
                    new_word.append(new_token)
                    i += 2  # Skip both tokens of the pair
                else:
                    new_word.append(word[i])
                    i += 1

            new_word_freq[tuple(new_word)] = freq

        return new_word_freq

    def train(self, text: str, num_merges: int) -> List[str]:
        """
        Train BPE on the given text for specified number of merges.
        Returns list of merge operations performed.
        """
        # Pre-tokenize text
        word_freq = self.pre_tokenize(text)
        print(f"Initial word frequencies: {self._format_word_freq(word_freq)}")

        merges_performed = []

        for merge_step in range(num_merges):
            # Get pair frequencies
            pair_freq = self.get_pair_frequencies(word_freq)

            if not pair_freq:
                break

            # Find most frequent pair (lexicographically largest in case of tie)
            max_freq = max(pair_freq.values())
            most_frequent_pairs = [pair for pair, freq in pair_freq.items() if freq == max_freq]

            # Sort pairs lexicographically - convert to comparable format
            def pair_sort_key(pair):
                def token_to_str(token):
                    if isinstance(token, int):
                        return chr(token)
                    return str(token)
                return (token_to_str(pair[0]), token_to_str(pair[1]))

            # Take lexicographically largest (max)
            best_pair = max(most_frequent_pairs, key=pair_sort_key)

            print(f"\nMerge {merge_step + 1}:")
            print(f"Pair frequencies: {self._format_pair_freq(pair_freq)}")
            print(f"Most frequent pair: {self._format_pair(best_pair)} (freq: {max_freq})")

            # Create new token name
            new_token = f"merge_{self.next_token_id}"

            # Perform the merge
            word_freq = self.merge_pair(word_freq, best_pair, new_token)

            # Record the merge
            token1_name = self._token_to_str(best_pair[0])
            token2_name = self._token_to_str(best_pair[1])
            merge_str = f"{token1_name} {token2_name}"
            merges_performed.append(merge_str)

            # Store merge information
            self.merges.append(best_pair)
            self.merge_tokens[best_pair] = new_token
            self.vocab[new_token] = self.next_token_id
            self.token_names[new_token] = merge_str
            self.next_token_id += 1

            print(f"After merge: {self._format_word_freq(word_freq)}")

        return merges_performed

    def tokenize(self, word: str) -> List[str]:
        """
        Tokenize a word using the learned BPE merges.
        """
        # Start with individual bytes as integers
        tokens = list(word.encode('utf-8'))

        # Apply merges in order
        for merge_pair in self.merges:
            new_tokens = []
            i = 0
            while i < len(tokens):
                if (i < len(tokens) - 1 and
                    tokens[i] == merge_pair[0] and
                    tokens[i + 1] == merge_pair[1]):
                    # Replace with the merged token name
                    merged_token = self.merge_tokens[merge_pair]
                    new_tokens.append(merged_token)
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens

        # Convert to readable format
        result = []
        for token in tokens:
            if isinstance(token, int):
                result.append(chr(token))
            elif isinstance(token, str) and token.startswith('merge_'):
                # Convert back to the original characters this merge represents
                result.append(self.token_names[token])
            else:
                result.append(str(token))

        return result

    def _format_word_freq(self, word_freq: Dict[Tuple[Union[int, str], ...], int]) -> str:
        """Format word frequency dictionary for readable output."""
        formatted = {}
        for word_tuple, freq in word_freq.items():
            tokens = [self._token_to_str(token) for token in word_tuple]
            word_str = '(' + ','.join(tokens) + ')'
            formatted[word_str] = freq
        return str(formatted)

    def _format_pair_freq(self, pair_freq: Dict[Tuple[Union[int, str], Union[int, str]], int]) -> str:
        """Format pair frequency dictionary for readable output."""
        formatted = {}
        for pair, freq in pair_freq.items():
            first = self._token_to_str(pair[0])
            second = self._token_to_str(pair[1])
            pair_str = first + second
            formatted[pair_str] = freq
        return str(formatted)

    def _format_pair(self, pair: Tuple[Union[int, str], Union[int, str]]) -> str:
        """Format a pair for readable output."""
        first = self._token_to_str(pair[0])
        second = self._token_to_str(pair[1])
        return f"({first}, {second})"

    def _token_to_str(self, token: Union[int, str]) -> str:
        """Convert a token to readable string."""
        if isinstance(token, int):
            return chr(token)
        elif isinstance(token, str) and token.startswith('merge_'):
            return self.token_names.get(token, token)
        else:
            return str(token)

# Example usage
if __name__ == "__main__":
    # Initialize BPE encoder
    bpe = BPEEncoder()

    # Example corpus
    corpus = "low low low low low lower lower widest widest widest newest newest newest newest newest newest"

    print("BPE Training on Corpus:")
    print(f"Corpus: {corpus}")
    print("=" * 50)

    # Train with 6 merges
    merges = bpe.train(corpus, num_merges=6)

    print("\n" + "=" * 50)
    print("Training Complete!")
    print(f"Merges performed: {merges}")

    # Test tokenization
    print("\n" + "=" * 50)
    print("Tokenization Examples:")
    test_words = ["newest", "lower", "widest", "low"]
    for word in test_words:
        tokens = bpe.tokenize(word)
        print(f"'{word}' -> {tokens}")

    # Show final vocabulary (subset)
    print("\n" + "=" * 50)
    print("New Vocabulary (merged tokens only):")
    for token, token_id in bpe.vocab.items():
        if isinstance(token, str) and token.startswith('merge_'):
            description = bpe.token_names[token]
            print(f"Token ID {token_id}: '{description}'")
```

---

## Sample Output

When you run this code, you'll see output like:

## ç¤ºä¾‹è¾“å‡º

å½“ä½ è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
    BPE Training on Corpus:
    Corpus: low low low low low lower lower widest widest widest newest newest newest newest newest newest
    ==================================================
    Initial word frequencies: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s,t)': 3, '(n,e,w,e,s,t)': 6}

    Merge 1:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es': 9, 'st': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (s, t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e,s t)': 3, '(n,e,w,e,s t)': 6}

    Merge 2:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 8, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es t': 9, 'ne': 6, 'ew': 6}
    Most frequent pair: (e, s t) (freq: 9)
    After merge: {'(l,o,w)': 5, '(l,o,w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}

    Merge 3:
    Pair frequencies: {'lo': 7, 'ow': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (o, w) (freq: 7)
    After merge: {'(l,o w)': 5, '(l,o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}

    Merge 4:
    Pair frequencies: {'lo w': 7, 'o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (l, o w) (freq: 7)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w,e s t)': 6}

    Merge 5:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew': 6, 'we s t': 6}
    Most frequent pair: (w, e s t) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n,e,w e s t)': 6}

    Merge 6:
    Pair frequencies: {'l o we': 2, 'er': 2, 'wi': 3, 'id': 3, 'de s t': 3, 'ne': 6, 'ew e s t': 6}
    Most frequent pair: (n, e) (freq: 6)
    After merge: {'(l o w)': 5, '(l o w,e,r)': 2, '(w,i,d,e s t)': 3, '(n e,w e s t)': 6}

    ==================================================
    Training Complete!
    Merges performed: ['s t', 'e s t', 'o w', 'l o w', 'w e s t', 'n e']

    ==================================================
    Tokenization Examples:
    'newest' -> ['n e', 'w e s t']
    'lower' -> ['l o w', 'e', 'r']
    'widest' -> ['w', 'i', 'd', 'e s t']
    'low' -> ['l o w']

    ==================================================
    New Vocabulary (merged tokens only):
    Token ID 257: 's t'
    Token ID 258: 'e s t'
    Token ID 259: 'o w'
    Token ID 260: 'l o w'
    Token ID 261: 'w e s t'
    Token ID 262: 'n e'
```

---

This implementation demonstrates how BPE learns to represent text efficiently by identifying and merging frequently occurring character patterns, creating a vocabulary that balances between the simplicity of byte-level tokenization and the efficiency of word-level tokenization.

è¿™ä¸ªå®ç°æ¼”ç¤ºäº†BPEå¦‚ä½•é€šè¿‡è¯†åˆ«å’Œåˆå¹¶é¢‘ç¹å‡ºç°çš„å­—ç¬¦æ¨¡å¼æ¥é«˜æ•ˆåœ°è¡¨ç¤ºæ–‡æœ¬ï¼Œåˆ›å»ºä¸€ä¸ªåœ¨å­—èŠ‚çº§åˆ†è¯çš„ç®€å•æ€§å’Œè¯çº§åˆ†è¯çš„æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡çš„è¯æ±‡è¡¨ã€‚
