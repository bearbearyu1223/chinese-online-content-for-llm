---
layout: post
title: "Study Notes: Stanford CS336 Language Modeling from Scratch [3a] - Building BPE Tokenizer (Part 1)"
categories: cs336
author:
- 大模型我都爱
---

<style>
  .xiaohongshu-link {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    color: #ff2442; /* 小红书主色 */
    text-decoration: none;
    font-weight: bold;
    font-size: 14px;
  }
  .xiaohongshu-link:hover {
    text-decoration: underline;
  }
  .xiaohongshu-logo {
    width: 18px;
    height: 18px;
    border-radius: 4px;
  }
</style>

<div style="padding:12px;border:1px solid #eee;border-radius:8px;display:inline-block;margin-bottom:20px;">
  <strong>大模型我都爱</strong><br>
  <p style="margin:4px 0;">
    小红书号：
    <a class="xiaohongshu-link"
       href="https://www.xiaohongshu.com/user/profile/5b2c5758e8ac2b08bf20e38d"
       target="_blank">
      <img class="xiaohongshu-logo"
           src="https://static.cdnlogo.com/logos/r/77/rednote-xiaohongshu.svg"
           alt="小红书 logo">
      119826921
    </a>
  </p>
  IP属地：美国
</div>

# Building a BPE Tokenizer from Scratch: Train the Tokenizer using TinyStories Dataset (Part 1)

# 从零开始构建BPE分词器：使用TinyStories数据集训练分词器（第1部分）

Ever wondered how modern language models like GPT break down text into tokens? In this note, I will share how to build a Byte Pair Encoding (BPE) tokenizer from scratch and train it on the [TinyStories Dataset](https://arxiv.org/abs/2305.07759). We will see how BPE achieves impressive compression ratios.

你是否好奇过像GPT这样的现代语言模型是如何将文本分解为词元（tokens）的？在这篇笔记中，我将分享如何从零开始构建一个字节对编码（BPE）分词器，并在[TinyStories数据集](https://arxiv.org/abs/2305.07759)上进行训练。我们将看到BPE如何实现令人印象深刻的压缩比。

## What is BPE Tokenization?

## 什么是BPE分词？

Byte Pair Encoding (BPE) is a compression algorithm that's become the backbone of modern tokenization. Here's how it works:

字节对编码（BPE）是一种压缩算法，已成为现代分词技术的支柱。它的工作原理如下：

1. **Start with bytes**: Every character becomes its byte representation (0-255)
2. **Find frequent pairs**: Look for the most common pair of adjacent tokens
3. **Merge and repeat**: Replace the most frequent pair with a new token, then repeat

1. **从字节开始**：每个字符都变成其字节表示（0-255）
2. **查找频繁配对**：寻找最常见的相邻词元对
3. **合并并重复**：用新词元替换最频繁的配对，然后重复此过程

### A Simple Example

### 一个简单的例子

Let's say we have the word "hello" appearing many times in our text:
- Initially: `h-e-l-l-o` (5 tokens)
- If "l-l" is the most frequent pair, merge it: `h-e-ll-o` (4 tokens)
- If "e-ll" becomes frequent, merge it: `h-ell-o` (3 tokens)

假设我们的文本中多次出现单词"hello"：
- 初始状态：`h-e-l-l-o`（5个词元）
- 如果"l-l"是最频繁的配对，则合并它：`h-e-ll-o`（4个词元）
- 如果"e-ll"变得频繁，则合并它：`h-ell-o`（3个词元）

This process creates a vocabulary that efficiently represents common patterns in your text. Check out [my previous post](https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html) for a brief introduction.

这个过程创建了一个词汇表，能够高效地表示文本中的常见模式。查看[我之前的文章](https://bearbearyu1223.github.io/cs336/2025/07/22/cs336-note-simple-bpe.html)获取简要介绍。

## The TinyStories Dataset

## TinyStories数据集

We'll train our tokenizer on [TinyStories](https://arxiv.org/abs/2305.07759), a fascinating dataset of short stories written using only words that 3-4 year olds typically understand. These stories were generated by GPT-3.5 and GPT-4, making them perfect for experimenting with tokenization.

我们将在[TinyStories](https://arxiv.org/abs/2305.07759)数据集上训练分词器，这是一个引人入胜的短篇故事数据集，仅使用3-4岁儿童通常能理解的词汇编写。这些故事由GPT-3.5和GPT-4生成，使它们成为分词实验的完美选择。

### Downloading the Data

### 下载数据

First, let's download the TinyStories froom Huggingface:

首先，让我们从Huggingface下载TinyStories数据集：

```python
!mkdir -p data
!cd data

!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
!gunzip owt_train.txt.gz
!wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
!gunzip owt_valid.txt.gz

!cd ..
```

## Challenge: Parallelizing Pre-tokenization

## 挑战：并行化预分词

The TinyStories dataset is big (over 2GB), which presents a challenge for tokenizer training. We need to:
1. Process the file in parallel for speed
2. Ensure we don't split tokens incorrectly at chunk boundaries

TinyStories数据集很大（超过2GB），这给分词器训练带来了挑战。我们需要：
1. 并行处理文件以提高速度
2. 确保在块边界处不会错误地分割词元

### Solution: Smart Chunking with Special Tokens

### 解决方案：使用特殊标记进行智能分块

Our solution uses special tokens (like `<|endoftext|>`) as natural boundaries for splitting the file.

我们的解决方案使用特殊标记（如`<|endoftext|>`）作为分割文件的自然边界。

**Simple Example**: Let's say we have a text file containing: "Hello`<SPLIT>`World`<SPLIT>`How`<SPLIT>`Are`<SPLIT>`You", special split token is `<SPLIT>`, and we want to divide the text into 3 chunks.

**简单示例**：假设我们有一个文本文件，内容为："Hello`<SPLIT>`World`<SPLIT>`How`<SPLIT>`Are`<SPLIT>`You"，特殊分割标记是`<SPLIT>`，我们想将文本分成3个块。

Here's one implementation for intelligent file chunking as shared in the [cs336 lecture notes](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py):

以下是[cs336课程笔记](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py)中分享的智能文件分块的一种实现：

```python
import os
from typing import BinaryIO

def find_chunk_boundaries( file: BinaryIO,
      desired_num_chunks: int,
      split_special_token: bytes)->list[int]:
  """
  Chunk the file into parts that can be counted independently.
  May return fewer chunks if the boundaries end up overlapping.
  """
  assert isinstance(split_special_token, bytes),(
      "Must represent special token as a bytestring"
  )

  # Get total file size in bytes
  file.seek(0, os.SEEK_END)
  file_size = file.tell()
  file.seek(0)

  chunk_size = file_size // desired_num_chunks

  # Initial guesses for chunk boundary locations, uniformly spaced
  # Chunks start on previous index, don't include last index
  chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]
  print(f"Initial guess of the chunk boundaries: {chunk_boundaries}")
  chunk_boundaries[-1] = file_size

  mini_chunk_size = 4096  # Read ahead by 4k bytes at a time

  for bi in range(1, len(chunk_boundaries) - 1):
      initial_position = chunk_boundaries[bi]
      file.seek(initial_position)  # Start at boundary guess
      while True:
          mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk

          # If EOF, this boundary should be at the end of the file
          if mini_chunk == b"":
              chunk_boundaries[bi] = file_size
              break

          # Find the special token in the mini chunk
          found_at = mini_chunk.find(split_special_token)
          if found_at != -1:
              chunk_boundaries[bi] = initial_position + found_at
              break
          initial_position += mini_chunk_size

  # Make sure all boundaries are unique, but might be fewer than desired_num_chunks
  return sorted(set(chunk_boundaries))
```

### Testing Our Chunking Algorithm

### 测试我们的分块算法

Let's see how this works with a concrete example:

让我们通过一个具体的例子来看看它是如何工作的：

```python
import io
def demonstrate_chunk_boundaries():
    """Demonstrate how to use find_chunk_boundaries with a practical example."""

    # Create sample data - our example text
    sample_text = "Hello<SPLIT>World<SPLIT>How<SPLIT>Are<SPLIT>You"
    sample_bytes = sample_text.encode('utf-8')

    print("=== Original Data ===")
    print(f"Text: {sample_text}")
    print(f"Bytes: {sample_bytes}")
    print(f"Total size: {len(sample_bytes)} bytes")
    print()

    # Create a file-like object from our sample data
    file_obj = io.BytesIO(sample_bytes)

    # Define our split token
    split_token = b"<SPLIT>"
    desired_chunks = 3

    print("=== Finding Chunk Boundaries ===")
    print(f"Desired number of chunks: {desired_chunks}")
    print(f"Split token: {split_token}")
    print()

    # Find the chunk boundaries
    boundaries = find_chunk_boundaries(file_obj, desired_chunks, split_token)

    print(f"Final boundaries: {boundaries}")
    print(f"Number of chunks created: {len(boundaries) - 1}")
    print()

    # Demonstrate how to use the boundaries to read chunks
    print("=== Reading Chunks ===")
    file_obj.seek(0)  # Reset file pointer

    for i in range(len(boundaries) - 1):
        start_pos = boundaries[i]
        end_pos = boundaries[i + 1]
        chunk_size = end_pos - start_pos

        # Read the chunk
        file_obj.seek(start_pos)
        chunk_data = file_obj.read(chunk_size)
        chunk_text = chunk_data.decode('utf-8')

        print(f"Chunk {i + 1}:")
        print(f"  Position: bytes {start_pos}-{end_pos-1}")
        print(f"  Size: {chunk_size} bytes")
        print(f"  Content: '{chunk_text}'")
        print(f"  Raw bytes: {chunk_data}")
        print()
```

Running this demonstration:

运行此演示：

```python
demonstrate_chunk_boundaries()
```

**Output:**

**输出：**

```
=== Original Data ===
Text: Hello<SPLIT>World<SPLIT>How<SPLIT>Are<SPLIT>You
Bytes: b'Hello<SPLIT>World<SPLIT>How<SPLIT>Are<SPLIT>You'
Total size: 47 bytes

=== Finding Chunk Boundaries ===
Desired number of chunks: 3
Split token: b'<SPLIT>'

Initial guess of the chunk boundaries: [0, 15, 30, 45]
Final boundaries: [0, 17, 37, 47]
Number of chunks created: 3

=== Reading Chunks ===
Chunk 1:
  Position: bytes 0-16
  Size: 17 bytes
  Content: 'Hello<SPLIT>World'
  Raw bytes: b'Hello<SPLIT>World'

Chunk 2:
  Position: bytes 17-36
  Size: 20 bytes
  Content: '<SPLIT>How<SPLIT>Are'
  Raw bytes: b'<SPLIT>How<SPLIT>Are'

Chunk 3:
  Position: bytes 37-46
  Size: 10 bytes
  Content: '<SPLIT>You'
  Raw bytes: b'<SPLIT>You'
```

Notice how the algorithm automatically adjusted the boundaries to align with `<SPLIT>` tokens, ensuring clean chunk separation.

注意算法如何自动调整边界以与`<SPLIT>`标记对齐，确保块的清晰分离。

## BPE Training Implementation

## BPE训练实现

Now implement the core BPE training algorithm. The implementation shared here handles parallel processing, special tokens, and efficient pair counting.

现在实现核心的BPE训练算法。这里分享的实现处理了并行处理、特殊标记和高效的配对计数。

### Core Training Function

### 核心训练函数

Here's is my complete BPE training implementation:

这是我完整的BPE训练实现：

```python
import re
import os
import multiprocessing as mp
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, BinaryIO

# Simplified GPT-2-style regex pattern for pre-tokenization (using standard re module)
GPT2_SPLIT_PATTERN = r"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-ZÀ-ÿ]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""

def process_chunk(args):
    """Process a chunk of the file and return word counts."""
    start, end, input_path, special_tokens = args
    word_counts = defaultdict(int)

    with open(input_path, 'rb') as f:
        f.seek(start)
        chunk = f.read(end - start).decode('utf-8', errors='ignore')

        # Split on special tokens to prevent merging across boundaries
        if special_tokens:
            pattern = '|'.join(re.escape(token) for token in special_tokens)
            text_segments = re.split(f'({pattern})', chunk)
        else:
            text_segments = [chunk]

        for segment in text_segments:
            if segment in special_tokens:
                continue  # Skip special tokens during counting

            # Apply GPT-2 regex pattern
            for match in re.finditer(GPT2_SPLIT_PATTERN, segment):
                token_text = match.group()
                token_bytes = tuple(token_text.encode('utf-8'))
                word_counts[token_bytes] += 1

    return word_counts


def train_bpe_tokenizer(input_path: str, vocab_size: int, special_tokens: list[str], verbose: bool = True) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """
    Train a byte-level Byte Pair Encoding (BPE) tokenizer from a text file.

    Args:
        input_path: Path to the input text file containing training data
        vocab_size: Maximum size of the final vocabulary (includes initial bytes + special tokens + merges)
        special_tokens: List of special token strings to include in vocabulary
        verbose: Whether to print training progress information

    Returns:
        vocab: Complete tokenizer vocabulary mapping token IDs to byte sequences
        merges: Ordered list of BPE merge operations performed during training
    """
    import time

    # Initialize vocabulary with bytes 0-255
    vocab = {i: bytes([i]) for i in range(256)}
    next_id = 256

    # Add special tokens to vocabulary
    for token in special_tokens:
        token_bytes = token.encode('utf-8')
        vocab[next_id] = token_bytes
        next_id += 1

    if verbose:
        print("Step 1: Setting up parallel processing...")

    # Get chunk boundaries for multiprocessing
    num_processes = mp.cpu_count()
    if verbose:
        print(f"Using {num_processes} processes for parallel tokenization")

    with open(input_path, 'rb') as f:
        if special_tokens:
            # Use first special token for chunking boundaries
            if verbose:
                print(f"Finding chunk boundaries aligned with special token: {special_tokens[0]}")
            boundaries = find_chunk_boundaries(f, num_processes, special_tokens[0].encode('utf-8'))
        else:
            # Use simple chunking without special token alignment
            f.seek(0, os.SEEK_END)
            file_size = f.tell()
            chunk_size = file_size // num_processes
            boundaries = [i * chunk_size for i in range(num_processes + 1)]
            boundaries[-1] = file_size
            if verbose:
                print(f"File size: {file_size:,} bytes, chunk size: {chunk_size:,} bytes")

    if verbose:
        print(f"Created {len(boundaries)-1} chunks for processing")
        print("\nStep 2: Pre-tokenizing text corpus...")

    # Process chunks in parallel
    chunk_args = []
    for start, end in zip(boundaries[:-1], boundaries[1:]):
        chunk_args.append((start, end, input_path, special_tokens))

    start_time = time.time()
    with mp.Pool(processes=num_processes) as pool:
        chunk_results = pool.map(process_chunk, chunk_args)
    tokenization_time = time.time() - start_time

    if verbose:
        print(f"Pre-tokenization completed in {tokenization_time:.2f} seconds")

    # Merge results from all chunks
    word_counts = defaultdict(int)
    total_tokens = 0
    for chunk_result in chunk_results:
        for word, count in chunk_result.items():
            word_counts[word] += count
            total_tokens += count

    if verbose:
        print(f"Found {len(word_counts):,} unique word types")
        print(f"Total token count: {total_tokens:,}")
        print(f"Most common words:")
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (word_bytes, count) in enumerate(sorted_words[:5]):
            try:
                word_str = bytes(word_bytes).decode('utf-8', errors='replace')
                print(f"  {i+1}. '{word_str}' -> {count:,} times")
            except:
                print(f"  {i+1}. {word_bytes} -> {count:,} times")

    # Convert to working format for BPE (list of byte values)
    word_freq = {}
    for word_bytes, freq in word_counts.items():
        word_tokens = list(word_bytes)  # Convert to list of ints
        word_freq[tuple(word_tokens)] = freq

    merges = []
    pair_index = {}  # Efficient indexing for pair counting

    def update_pair_index(word_freq, pair_index):
        """Update the pair index for efficient counting."""
        pair_index.clear()
        for word, freq in word_freq.items():
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                if pair not in pair_index:
                    pair_index[pair] = []
                pair_index[pair].append((word, i, freq))

    def count_pairs(pair_index):
        """Count pair frequencies efficiently using the index."""
        pair_counts = defaultdict(int)
        for pair, occurrences in pair_index.items():
            total_count = sum(freq for _, _, freq in occurrences)
            pair_counts[pair] = total_count
        return pair_counts

    # BPE training loop
    target_merges = vocab_size - len(vocab)

    if verbose:
        print(f"\nStep 3: Training BPE with {target_merges:,} merges...")
        print(f"Initial vocabulary size: {len(vocab)} (256 bytes + {len(special_tokens)} special tokens)")
        print("=" * 60)

    bpe_start_time = time.time()

    for merge_num in range(target_merges):
        merge_step_start = time.time()

        # Update pair index
        update_pair_index(word_freq, pair_index)

        # Count pairs efficiently
        pair_counts = count_pairs(pair_index)

        if not pair_counts:
            if verbose:
                print(f"No more pairs to merge at step {merge_num + 1}")
            break

        # Find most frequent pair (with lexicographic tiebreaking)
        best_pair = max(pair_counts.items(), key=lambda x: (x[1], x[0]))[0]
        best_count = pair_counts[best_pair]

        # Create new token for merge
        new_token_id = next_id
        next_id += 1

        # Get the byte sequences for the tokens being merged
        left_bytes = vocab[best_pair[0]]
        right_bytes = vocab[best_pair[1]]

        # Record merge as byte sequences
        merges.append((left_bytes, right_bytes))

        # Update vocabulary - merge the two byte sequences
        vocab[new_token_id] = left_bytes + right_bytes

        # Update word frequencies by applying merge
        new_word_freq = {}

        for word, freq in word_freq.items():
            new_word = []
            i = 0
            while i < len(word):
                if (i < len(word) - 1 and
                    word[i] == best_pair[0] and
                    word[i + 1] == best_pair[1]):
                    new_word.append(new_token_id)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            new_word_tuple = tuple(new_word)
            if new_word_tuple in new_word_freq:
                new_word_freq[new_word_tuple] += freq
            else:
                new_word_freq[new_word_tuple] = freq

        word_freq = new_word_freq
        merge_step_time = time.time() - merge_step_start

        # Progress logging
        if verbose:
            if (merge_num + 1) % 100 == 0 or merge_num < 10 or (merge_num + 1) % 1000 == 0:
                try:
                    left_str = left_bytes.decode('utf-8', errors='replace')
                    right_str = right_bytes.decode('utf-8', errors='replace')
                    merged_str = (left_bytes + right_bytes).decode('utf-8', errors='replace')
                    print(f"Merge {merge_num + 1:4d}/{target_merges}: "
                          f"'{left_str}' + '{right_str}' -> '{merged_str}' "
                          f"(freq: {best_count:,}, time: {merge_step_time:.3f}s)")
                except:
                    print(f"Merge {merge_num + 1:4d}/{target_merges}: "
                          f"{left_bytes} + {right_bytes} -> {left_bytes + right_bytes} "
                          f"(freq: {best_count:,}, time: {merge_step_time:.3f}s)")

    bpe_time = time.time() - bpe_start_time

    if verbose:
        print("=" * 60)
        print(f"BPE training completed in {bpe_time:.2f} seconds")
        print(f"Final vocabulary size: {len(vocab)}")
        print(f"Total merges performed: {len(merges)}")

        # Show compression statistics
        if word_counts:
            original_tokens = sum(len(bytes(word_bytes)) for word_bytes, count in word_counts.items() for _ in range(count))
            compressed_tokens = sum(len(word) for word, count in word_freq.items() for _ in range(count))
            compression_ratio = original_tokens / compressed_tokens if compressed_tokens > 0 else 1.0
            print(f"Compression ratio: {compression_ratio:.2f}x (from {original_tokens:,} to {compressed_tokens:,} tokens)")

    return vocab, merges


def save_tokenizer(vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]],
                  vocab_path: str, merges_path: str):
    """Save vocabulary and merges to disk files."""
    import json
    import pickle

    # Save vocabulary
    with open(vocab_path, 'wb') as f:
        pickle.dump(vocab, f)

    # Save merges
    with open(merges_path, 'wb') as f:
        pickle.dump(merges, f)


def load_tokenizer(vocab_path: str, merges_path: str) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """Load vocabulary and merges from disk files."""
    import pickle

    with open(vocab_path, 'rb') as f:
        vocab = pickle.load(f)

    with open(merges_path, 'rb') as f:
        merges = pickle.load(f)

    return vocab, merges
```

---

Continue reading in [Part 2](/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/) for training results and testing examples.

继续阅读[第2部分](/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/)以查看训练结果和测试示例。
