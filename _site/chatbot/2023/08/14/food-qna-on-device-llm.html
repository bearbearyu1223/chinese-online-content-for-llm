<!DOCTYPE html>
<html lang="en"><head>
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Build On-Device QnA with LangChain and Llama2 | ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Build On-Device QnA with LangChain and Llama2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TL;DR This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (llama-2-7b-chat.ggmlv3.q8_0.bin), an embedding model (sentence-transformers/all-MiniLM-L6-v2), and an on-device vector database (FAISS). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference." />
<meta property="og:description" content="TL;DR This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (llama-2-7b-chat.ggmlv3.q8_0.bin), an embedding model (sentence-transformers/all-MiniLM-L6-v2), and an on-device vector database (FAISS). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference." />
<link rel="canonical" href="http://localhost:4000/chinese-online-content-for-llm/chatbot/2023/08/14/food-qna-on-device-llm.html" />
<meta property="og:url" content="http://localhost:4000/chinese-online-content-for-llm/chatbot/2023/08/14/food-qna-on-device-llm.html" />
<meta property="og:site_name" content="ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-14T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Build On-Device QnA with LangChain and Llama2" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-08-14T00:00:00-07:00","datePublished":"2023-08-14T00:00:00-07:00","description":"TL;DR This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (llama-2-7b-chat.ggmlv3.q8_0.bin), an embedding model (sentence-transformers/all-MiniLM-L6-v2), and an on-device vector database (FAISS). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.","headline":"Build On-Device QnA with LangChain and Llama2","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/chinese-online-content-for-llm/chatbot/2023/08/14/food-qna-on-device-llm.html"},"url":"http://localhost:4000/chinese-online-content-for-llm/chatbot/2023/08/14/food-qna-on-device-llm.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chinese-online-content-for-llm/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/chinese-online-content-for-llm/feed.xml" title="ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±" /><link rel="stylesheet" href="/chinese-online-content-for-llm/assets/css/dark-mode.css">
<script src="/chinese-online-content-for-llm/assets/js/theme-toggle.js"></script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chinese-online-content-for-llm/">ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chinese-online-content-for-llm/cs336/">ğŸ“š Stanford CS336</a><a class="page-link" href="/chinese-online-content-for-llm/career/">ğŸŒ± Career &amp; Growth</a><a class="page-link" href="/chinese-online-content-for-llm/ai-insights/">ğŸ§¬ AI Insights</a><a class="page-link" href="/chinese-online-content-for-llm/about/">å…³äºæœ¬ç«™</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Build On-Device QnA with LangChain and Llama2</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-08-14T00:00:00-07:00" itemprop="datePublished">
        Aug 14, 2023
      </time>â€¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Han Yu</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="tldr">TL;DR</h3>
<p>This post involves creating a Question and Answering system using the LLM model hosted on Apple M1 Pro. The key building blocks include the LLM model (<a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML">llama-2-7b-chat.ggmlv3.q8_0.bin</a>), an embedding model (<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">sentence-transformers/all-MiniLM-L6-v2</a>), and an on-device vector database (<a href="https://github.com/facebookresearch/faiss">FAISS</a>). The app is built using the â€œLangChainâ€ framework. All components are open source, eliminating the need for OpenAI services. The systemâ€™s performance is similar to OpenAI but with 10x longer latency (around 60s/query vs 5s/query) due to on-device model inference.</p>

<h3 id="introduction">Introduction</h3>
<p>Third-party commercial large language model (LLM) providers, such as OpenAIâ€™s GPT-4, Google Bard, and Amazon AlexaTM, have greatly democratized access to LLM capabilities through seamless API integration and scalable model inference hosting in the cloud. These advanced LLMs possess the remarkable ability to <strong>comprehend</strong>, <strong>learn from</strong>, and <strong>produce text</strong> that is nearly indistinguishable from human-generated content. Beyond their text generation prowess, these LLMs excel in <strong>interactive conversations</strong>, <strong>question answering</strong>, <strong>dialogue</strong> and <strong>document summarization</strong>, as well as <strong>offering insightful recommendations</strong>. Their versatility finds applications across diverse tasks and industries including creative copywriting for marketing, precise document summarization for legal purposes, data-driven market research in the financial sector, realistic simulation of clinical trials within healthcare, and even code generation for software development.</p>

<p>However, certain scenarios, driven by an increasing emphasis on safeguarding data privacy and adhering to stringent regulatory compliance standards, highlight the necessity of deploying LLMs on private hardware devices instead of on any of those third-party owned servers. In such instances, <strong>maintaining sensitive information within the confines of the userâ€™s hardware</strong> not only mitigates the risks associated with data breaches and unauthorized access but also aligns with the evolving landscape of privacy-conscious technical practices. This approach <strong>fosters a sense of trust among users who are becoming more attuned to the importance of maintaining their personal information within their own environments</strong>.</p>

<p>In this post, our focus lies in exploring the execution of quantized variants of open-source Llama2 models on local devices to achieve Retrieval Augmented Generation (RAG). For RAG powered by server-side LLMs, you can find more info in <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">my previous post</a>.</p>

<h3 id="llama2-and-its-variants">Llama2 and Its variants</h3>
<p><a href="https://ai.meta.com/resources/models-and-libraries/llama/">Llama 2</a>, launched by Meta in July 2023, has been pretrained on publicly available online data sources, encompassing a staggering <strong>2 trillion tokens with a context length of 4096</strong>. The subsequent supervised fine-tuned iteration of this model, known as Llama-2-chat, underwent meticulous refinement through the integration of over <em>1 million human annotations</em> to cater specifically to <strong>chat-oriented use cases</strong>. Meta has extended the accessibility of Llama 2 to a wide spectrum of users, ranging from individual developers and content creators to researchers and businesses. This strategic open-source initiative is aimed at fostering an ecosystem conducive to <a href="https://ai.meta.com/static-resource/responsible-use-guide/">Responsible AI experimentation</a>, innovation, and the scalable implementation of a diverse array of ideas, thus further <strong>democratizing Generative AI</strong>.</p>

<p>Llama 2 is offered in an array of parameter sizes â€” 7B, 13B, and 70B â€” alongside both pretrained and fine-tuned variations to cater to a wide range of application needs.</p>

<h3 id="framework-and-libraries-used-langchain-ggml-c-transformers">Framework and Libraries Used: LangChain, GGML, C Transformers</h3>
<p><a href="https://python.langchain.com/docs/get_started/introduction.html">LangChain</a> is an open source framework for developing applications powered by LLMs. It goes beyond standard API calls by being <em>data-aware</em>, enabling connections with various data sources for richer, personalized experiences. It is also <em>agentic</em>, meaning it can empower a language model to interact dynamically with its environment. LangChain streamlines the development of diverse applications, such as chatbots, Generative Question and Answering (GQA), and summarization. By â€œchainingâ€ components from multiple modules, it allows for the creation of unique applications built around an LLM with <strong>easy-to-code</strong> and <strong>fast-to-production</strong> developer experience.</p>

<p><a href="https://github.com/ggerganov/ggml">GGML</a> is a C library for machine learning (ML). GGML makes use of a technique called <strong>â€œquantizationâ€</strong> (e.g., convert LLMâ€™s weights from high-precison floating numbers to low-precision floating numbers) that allows for large language models to run on consumer hardware. GGML supports a number of different quantization strategies (e.g. 4-bit, 5-bit, and 8-bit quantization), each of which offers different <em>trade-offs between efficiency and performance</em>. More information about these trade-offs (such as model disk size and inference speed) can be found in <a href="https://github.com/ggerganov/llama.cpp">the documentation for llama.cpp</a>.</p>

<p><a href="https://github.com/marella/ctransformers">C Transformers</a> is a wrapper that provides the Python bindings for the Transformer models implemented in C/C++ using GGML. 
C Transformers supports running Llama2 model inference via GPU, for both NVIDIA GPU (via CUDA, a programming language for NVIDIA GPUs) and Appleâ€™s own integreated GPU and Neural Engine (via Metal, a programming language for Apple integrated GPUs).</p>

<p>Note: To use C transformers with Metal Support for model inference running on Apple M1/M2 chip, need run the following cmd under your project root</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry config --local installer.no-binary ctransformers

poetry add ctransformers 
</code></pre></div></div>

<h3 id="retrieval-augmented-generation">Retrieval Augmented Generation</h3>
<p>Retrieval Augmented Generation (RAG) represents a technique wherein data is retrieved from external sources to enhance and expand the prompts used in model generation. This method is not only a cost-effective alternative but also proves to be an efficient approach in comparison to the traditional methods of pre-training or fine-tuning foundation models.
See the previous post at <a href="https://bearbearyu1223.github.io/chatbot/2023/07/31/food-qna-on-server-llm.html">Food QnA Chatbot : Help Answer Food Related Questions from Your Own Cookbook</a> as a brief into to RAG.</p>

<h3 id="an-example-project">An Example Project</h3>
<p>The source code for the example project can be found on <a href="https://github.com/bearbearyu1223/langchain_playground/tree/main/food_qna_on_device"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a>. The project directory should look like below:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>food_qna_on_device
â”œâ”€â”€ README.md
â”œâ”€â”€ build_knowledge_base.py
â”œâ”€â”€ config.py
â”œâ”€â”€ cook_book_data
â”‚   â”œâ”€â”€ GCE-Dinner-in-30-EXPANDED-BLAD.pdf
â”‚   â”œâ”€â”€ Quick-Easy-Weeknight-Meals-1.pdf
â”‚   â””â”€â”€ dinners_cookbook_508-compliant.pdf
â”œâ”€â”€ main.py
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ llama-2-13b-chat.ggmlv3.q8_0.bin
â”‚   â””â”€â”€ llama-2-7b-chat.ggmlv3.q8_0.bin
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ poetry.toml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ vector_db
    â”œâ”€â”€ index.faiss
    â””â”€â”€ index.pkl
</code></pre></div></div>

<p>Instruction to run the example project:</p>
<ul>
  <li>Step 1: Launch the terminal from the project directory, install and resolve the dependencies as defined in <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file via
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry install
</code></pre></div>    </div>
  </li>
  <li>Step 2: Download the quantized 7b model <code class="language-plaintext highlighter-rouge">llama-2-7b-chat.ggmlv3.q8_0.bin</code> from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML and save the model under the directory <code class="language-plaintext highlighter-rouge">models\</code></li>
  <li>Step 3: To start parsing user queries into the application, run the following command from the project directory (note: the model inference can take ~1 mins per input query)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c local 
</code></pre></div>    </div>
    <p>Optionally, to run the same query with OpenAI (note: the model inference will take a few seconds per input query, you will also need export OPENAI_API_KEY as an enviroment variable on your local dev machine)</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poetry run python main.py -c server
</code></pre></div>    </div>
  </li>
  <li>Step 4: Enter a query related to food preparation and cooking into the console and start playing with it. 
See an example below:
Screenshot of the original content for making â€œPumpkin Biscuitsâ€. 
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/original.png" alt="Original Content" />
Retrieval Augmented Generation by running Llama2 model inference on local device
 <img src="/assets/picture/2023_08_14_food_qna_on_device_llm/response.png" alt="Response" /></li>
</ul>

  </div><a class="u-url" href="/chinese-online-content-for-llm/chatbot/2023/08/14/food-qna-on-device-llm.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chinese-online-content-for-llm/"></data>

  <div class="wrapper">
    <!-- Support Section -->
    <div class="footer-support-section">
      <h3>â˜• æ”¯æŒæˆ‘çš„åˆ›ä½œ Support My Work</h3>
      <p style="margin-bottom: 1em; opacity: 0.9;">å¦‚æœè¿™äº›å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿è¯·æˆ‘å–æ¯å’–å•¡ï¼<br>If you find my content helpful, consider buying me a coffee!</p>
      <div class="support-options">
        <a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" class="support-btn buymeacoffee-btn">
          <span class="btn-icon">â˜•</span>
          <span class="btn-text">Buy Me a Coffee</span>
        </a>
      </div>
    </div>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chinese-online-content-for-llm/feed.xml">
            <svg class="svg-icon orange"><use xlink:href="/chinese-online-content-for-llm/assets/minima-social-icons.svg#rss"></use></svg><span>Subscribe</span>
          </a>
        </p>
        <p>A curated collection of Chinese online content designed for Large Language Model training, evaluation, and research. Exploring high-quality Chinese language resources for multilingual AI development.
</p>
      </div>
      <div class="footer-col">
        <p>Â© 2025 ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</p>
      </div>
    </div>
  </div>

  <!-- Theme toggle button -->
  <button id="theme-toggle" aria-label="Toggle dark mode">ğŸŒ™</button>
</footer>

<style>
.footer-support-section {
  text-align: center;
  padding: 2em 0;
  margin-bottom: 2em;
  border-bottom: 1px solid var(--border-color);
}

.footer-support-section h3 {
  margin: 0 0 0.5em 0;
  color: var(--text-color);
}

.support-options {
  display: flex;
  gap: 1em;
  justify-content: center;
  flex-wrap: wrap;
}

.support-btn {
  display: inline-flex;
  align-items: center;
  gap: 0.5em;
  padding: 0.75em 1.5em;
  border: none;
  border-radius: 8px;
  font-size: 1em;
  font-weight: 600;
  text-decoration: none;
  cursor: pointer;
  transition: all 0.3s ease;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.support-btn:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0,0,0,0.15);
}

.buymeacoffee-btn {
  background: #FFDD00;
  color: #000;
}

.buymeacoffee-btn:hover {
  background: #FFE633;
}

.btn-icon {
  font-size: 1.2em;
}

[data-theme="dark"] .footer-support-section {
  border-bottom-color: #444;
}

[data-theme="dark"] .support-btn {
  box-shadow: 0 2px 8px rgba(0,0,0,0.3);
}

[data-theme="dark"] .support-btn:hover {
  box-shadow: 0 4px 12px rgba(0,0,0,0.4);
}
</style>
<!-- Floating Support Button -->
<a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" id="floating-support-btn" class="floating-support-btn" aria-label="Support this site">
  <span class="support-heart">â¤ï¸</span>
  <span class="support-text">Support</span>
</a>

<!-- Support Modal -->
<div id="support-modal" class="support-modal" onclick="if(event.target === this) this.style.display='none'">
  <div class="support-modal-content">
    <button class="modal-close" onclick="document.getElementById('support-modal').style.display='none'" aria-label="Close modal">&times;</button>

    <h2 style="text-align: center; margin-top: 0;">â˜• æ”¯æŒæˆ‘çš„åˆ›ä½œ</h2>
    <p style="text-align: center; color: #666; margin-bottom: 2em;">
      å¦‚æœè¿™äº›å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿æ”¯æŒæˆ‘ç»§ç»­åˆ›ä½œï¼<br>
      <small>If you find my content helpful, please consider supporting my work!</small>
    </p>

    <div class="support-methods">
      <!-- Buy Me a Coffee -->
      <div class="support-method">
        <div class="method-icon" style="background: #FFDD00;">â˜•</div>
        <h3>Buy Me a Coffee</h3>
        <p>One-time support with PayPal, Card, etc.</p>
        <a href="https://buymeacoffee.com/YOUR_USERNAME" target="_blank" rel="noopener" class="method-btn" style="background: #FFDD00; color: #000;">
          Support on Buy Me a Coffee
        </a>
      </div>

      <!-- Ko-fi -->
      <div class="support-method">
        <div class="method-icon" style="background: #13C3FF; color: white;">ğŸ’™</div>
        <h3>Ko-fi</h3>
        <p>Zero fees! Support with PayPal or Card</p>
        <a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" class="method-btn" style="background: #13C3FF; color: white;">
          Support on Ko-fi
        </a>
      </div>

      <!-- GitHub Sponsors (Optional) -->
      <div class="support-method">
        <div class="method-icon" style="background: #24292e; color: white;">â­</div>
        <h3>GitHub Sponsors</h3>
        <p>Monthly sponsorship for tech creators</p>
        <a href="https://github.com/sponsors/YOUR_USERNAME" target="_blank" rel="noopener" class="method-btn" style="background: #24292e; color: white;">
          Sponsor on GitHub
        </a>
      </div>
    </div>

    <p style="text-align: center; margin-top: 2em; color: #888; font-size: 0.9em;">
      æ„Ÿè°¢ä½ çš„æ”¯æŒï¼ä½ çš„æ¯ä¸€ä»½å¿ƒæ„éƒ½æ˜¯æˆ‘æŒç»­åˆ›ä½œçš„åŠ¨åŠ› â¤ï¸<br>
      <small>Thank you for your support! Every contribution motivates me to create more content.</small>
    </p>
  </div>
</div>

<style>
/* Floating Support Button */
.floating-support-btn {
  position: fixed;
  bottom: 30px;
  right: 30px;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border: none;
  border-radius: 50px;
  padding: 12px 24px;
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  box-shadow: 0 4px 16px rgba(102, 126, 234, 0.4);
  z-index: 999;
  display: flex;
  align-items: center;
  gap: 8px;
  transition: all 0.3s ease;
  text-decoration: none;
}

.floating-support-btn:hover {
  transform: translateY(-3px);
  box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
  color: white;
  text-decoration: none;
}

.support-heart {
  font-size: 1.2em;
  animation: heartbeat 1.5s ease-in-out infinite;
}

@keyframes heartbeat {
  0%, 100% { transform: scale(1); }
  25% { transform: scale(1.1); }
  50% { transform: scale(1); }
}

/* Support Modal */
.support-modal {
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.7);
  z-index: 1000;
  justify-content: center;
  align-items: center;
  padding: 20px;
  overflow-y: auto;
}

.support-modal-content {
  background: var(--bg-color);
  border-radius: 16px;
  padding: 2em;
  max-width: 900px;
  width: 100%;
  position: relative;
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
  max-height: 90vh;
  overflow-y: auto;
}

.modal-close {
  position: absolute;
  top: 15px;
  right: 15px;
  background: none;
  border: none;
  font-size: 2em;
  color: var(--text-color);
  cursor: pointer;
  line-height: 1;
  padding: 0;
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  transition: background 0.3s ease;
}

.modal-close:hover {
  background: rgba(0, 0, 0, 0.1);
}

.support-methods {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 1.5em;
  margin: 2em 0;
}

.support-method {
  border: 2px solid var(--border-color);
  border-radius: 12px;
  padding: 1.5em;
  text-align: center;
  background: var(--bg-color);
  transition: all 0.3s ease;
}

.support-method:hover {
  transform: translateY(-5px);
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.1);
  border-color: #667eea;
}

.method-icon {
  width: 60px;
  height: 60px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 2em;
  margin: 0 auto 1em auto;
}

.support-method h3 {
  margin: 0.5em 0;
  color: var(--text-color);
}

.support-method p {
  color: #666;
  font-size: 0.9em;
  margin: 0.5em 0 1em 0;
  min-height: 2.5em;
}

.method-btn {
  display: inline-block;
  padding: 0.75em 1.5em;
  border-radius: 8px;
  text-decoration: none;
  font-weight: 600;
  transition: all 0.3s ease;
}

.method-btn:hover {
  transform: scale(1.05);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

/* Dark mode adjustments */
[data-theme="dark"] .modal-close:hover {
  background: rgba(255, 255, 255, 0.1);
}

[data-theme="dark"] .support-method {
  background: #1a1a1a;
}

[data-theme="dark"] .support-method p {
  color: #a0aec0;
}

/* Responsive design */
@media (max-width: 768px) {
  .floating-support-btn {
    bottom: 20px;
    right: 20px;
    padding: 10px 20px;
    font-size: 14px;
  }

  .support-modal-content {
    padding: 1.5em;
    margin: 10px;
  }

  .support-methods {
    grid-template-columns: 1fr;
  }
}

@media (max-width: 480px) {
  .floating-support-btn {
    bottom: 15px;
    right: 15px;
    padding: 8px 16px;
    font-size: 13px;
  }

  .support-text {
    display: none;
  }
}
</style>
</body>

</html>
