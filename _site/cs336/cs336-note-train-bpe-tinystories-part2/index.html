<!DOCTYPE html>
<html lang="en"><head>
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Study Notes: Stanford CS336 Language Modeling from Scratch [3b] - Building BPE Tokenizer (Part 2) | ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [3b] - Building BPE Tokenizer (Part 2)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A curated collection of Chinese online content designed for Large Language Model training, evaluation, and research. Exploring high-quality Chinese language resources for multilingual AI development." />
<meta property="og:description" content="A curated collection of Chinese online content designed for Large Language Model training, evaluation, and research. Exploring high-quality Chinese language resources for multilingual AI development." />
<link rel="canonical" href="http://localhost:4000/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/" />
<meta property="og:url" content="http://localhost:4000/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/" />
<meta property="og:site_name" content="ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-26T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Study Notes: Stanford CS336 Language Modeling from Scratch [3b] - Building BPE Tokenizer (Part 2)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-26T00:00:00-07:00","datePublished":"2025-07-26T00:00:00-07:00","description":"A curated collection of Chinese online content designed for Large Language Model training, evaluation, and research. Exploring high-quality Chinese language resources for multilingual AI development.","headline":"Study Notes: Stanford CS336 Language Modeling from Scratch [3b] - Building BPE Tokenizer (Part 2)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/"},"url":"http://localhost:4000/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chinese-online-content-for-llm/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/chinese-online-content-for-llm/feed.xml" title="ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±" /><link rel="stylesheet" href="/chinese-online-content-for-llm/assets/css/dark-mode.css">
<script src="/chinese-online-content-for-llm/assets/js/theme-toggle.js"></script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chinese-online-content-for-llm/">ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chinese-online-content-for-llm/cs336/">ğŸ“š Stanford CS336</a><a class="page-link" href="/chinese-online-content-for-llm/career/">ğŸŒ± Career &amp; Growth</a><a class="page-link" href="/chinese-online-content-for-llm/ai-insights/">ğŸ§¬ AI Insights</a><a class="page-link" href="/chinese-online-content-for-llm/about/">å…³äºæœ¬ç«™</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Study Notes: Stanford CS336 Language Modeling from Scratch [3b] - Building BPE Tokenizer (Part 2)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-07-26T00:00:00-07:00" itemprop="datePublished">
        Jul 26, 2025
      </time>â€¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <style>
  .xiaohongshu-link {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    color: #ff2442; /* å°çº¢ä¹¦ä¸»è‰² */
    text-decoration: none;
    font-weight: bold;
    font-size: 14px;
  }
  .xiaohongshu-link:hover {
    text-decoration: underline;
  }
  .xiaohongshu-logo {
    width: 18px;
    height: 18px;
    border-radius: 4px;
  }
</style>

<div style="padding:12px;border:1px solid #eee;border-radius:8px;display:inline-block;margin-bottom:20px;">
  <strong>å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</strong><br />
  <p style="margin:4px 0;">
    å°çº¢ä¹¦å·ï¼š
    <a class="xiaohongshu-link" href="https://www.xiaohongshu.com/user/profile/5b2c5758e8ac2b08bf20e38d" target="_blank">
      <img class="xiaohongshu-logo" src="https://static.cdnlogo.com/logos/r/77/rednote-xiaohongshu.svg" alt="å°çº¢ä¹¦ logo" />
      119826921
    </a>
  </p>
  IPå±åœ°ï¼šç¾å›½
</div>

<h1 id="building-a-bpe-tokenizer-from-scratch-training-results-and-testing-part-2">Building a BPE Tokenizer from Scratch: Training Results and Testing (Part 2)</h1>

<h1 id="ä»é›¶å¼€å§‹æ„å»ºbpeåˆ†è¯å™¨è®­ç»ƒç»“æœå’Œæµ‹è¯•ç¬¬2éƒ¨åˆ†">ä»é›¶å¼€å§‹æ„å»ºBPEåˆ†è¯å™¨ï¼šè®­ç»ƒç»“æœå’Œæµ‹è¯•ï¼ˆç¬¬2éƒ¨åˆ†ï¼‰</h1>

<p>This is Part 2 of building a BPE tokenizer. See <a href="/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part1/">Part 1</a> for the implementation details including the chunking algorithm and BPE training code.</p>

<p>è¿™æ˜¯æ„å»ºBPEåˆ†è¯å™¨çš„ç¬¬2éƒ¨åˆ†ã€‚æŸ¥çœ‹<a href="/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part1/">ç¬¬1éƒ¨åˆ†</a>äº†è§£å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬åˆ†å—ç®—æ³•å’ŒBPEè®­ç»ƒä»£ç ã€‚</p>

<h2 id="training-on-tinystories-dataset">Training on TinyStories Dataset</h2>

<h2 id="åœ¨tinystoriesæ•°æ®é›†ä¸Šè®­ç»ƒ">åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒ</h2>

<p>Now letâ€™s use our implementation to train a tokenizer on the TinyStories dataset. Here is one training function to demonstrate all the steps:</p>

<p>ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„å®ç°åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒåˆ†è¯å™¨ã€‚è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºæ‰€æœ‰æ­¥éª¤çš„è®­ç»ƒå‡½æ•°ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER TRAINING ON TINYSTORIES DATASET"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Configuration
</span>    <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

    <span class="c1"># Check if input file exists
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="n">input_path</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: Input file '</span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">' not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please ensure the TinyStories dataset is in the data/ directory."</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Display configuration
</span>    <span class="n">file_size</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">input_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Configuration:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Input file: </span><span class="si">{</span><span class="n">input_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  File size: </span><span class="si">{</span><span class="n">file_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">file_size</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> MB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Target vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Verbose logging: Enabled"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Train the tokenizer with verbose output
</span>    <span class="n">overall_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">train_bpe_tokenizer</span><span class="p">(</span>
        <span class="n">input_path</span><span class="o">=</span><span class="n">input_path</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Enable detailed logging
</span>    <span class="p">)</span>
    <span class="n">overall_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING SUMMARY"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total training time: </span><span class="si">{</span><span class="n">overall_end_time</span> <span class="o">-</span> <span class="n">overall_start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Final vocabulary size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of merges performed: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Actual vocab size vs target: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Save the tokenizer
</span>    <span class="n">vocab_path</span> <span class="o">=</span> <span class="s">"tinystories_vocab.pkl"</span>
    <span class="n">merges_path</span> <span class="o">=</span> <span class="s">"tinystories_merges.pkl"</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Saving tokenizer to disk..."</span><span class="p">)</span>
    <span class="n">save_tokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">,</span> <span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  âœ“ Vocabulary saved to: </span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  âœ“ Merges saved to: </span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Detailed vocabulary analysis
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY ANALYSIS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

    <span class="c1"># Count different types of tokens
</span>    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">token_id</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_token_count</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token type breakdown:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Special tokens: </span><span class="si">{</span><span class="n">special_token_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some vocabulary examples
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Byte tokens (first 10):"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">char</span><span class="p">.</span><span class="n">isprintable</span><span class="p">()</span> <span class="ow">and</span> <span class="n">char</span> <span class="o">!=</span> <span class="s">' '</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">char</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">char</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Special tokens:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token_str</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">:</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">token_str</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">token_id</span><span class="p">,</span> <span class="n">vocab_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">vocab_bytes</span> <span class="o">==</span> <span class="n">token_bytes</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab_bytes</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Most recently merged tokens (last 10):"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">256</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">special_tokens</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">merged_token_ids</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">].</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span><span class="si">}</span><span class="s"> -&gt; (non-UTF8)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">First 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Last 10 merge operations:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:],</span> <span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">9</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">left_str</span> <span class="o">=</span> <span class="n">left</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">right_str</span> <span class="o">=</span> <span class="n">right</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="n">merged_str</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">).</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">left_str</span><span class="si">}</span><span class="s">' + '</span><span class="si">{</span><span class="n">right_str</span><span class="si">}</span><span class="s">' -&gt; '</span><span class="si">{</span><span class="n">merged_str</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merge </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">left</span><span class="si">}</span><span class="s"> + </span><span class="si">{</span><span class="n">right</span><span class="si">}</span><span class="s"> -&gt; (binary)"</span><span class="p">)</span>

    <span class="c1"># Show file sizes
</span>    <span class="n">vocab_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span>
    <span class="n">merges_size_bytes</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">merges_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Output file sizes:"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Vocabulary file: </span><span class="si">{</span><span class="n">vocab_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Merges file: </span><span class="si">{</span><span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="n">merges_size_bytes</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Total: </span><span class="si">{</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> bytes (</span><span class="si">{</span><span class="p">(</span><span class="n">vocab_size_bytes</span> <span class="o">+</span> <span class="n">merges_size_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> KB)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"TRAINING COMPLETED SUCCESSFULLY!"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"You can now use the trained tokenizer for encoding/decoding text."</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Load with: vocab, merges = load_tokenizer('</span><span class="si">{</span><span class="n">vocab_path</span><span class="si">}</span><span class="s">', '</span><span class="si">{</span><span class="n">merges_path</span><span class="si">}</span><span class="s">')"</span><span class="p">)</span>
</code></pre></div></div>

<p>To run the training, one can try for example:</p>

<p>è¦è¿è¡Œè®­ç»ƒï¼Œå¯ä»¥å°è¯•ä¾‹å¦‚ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_bpe_tokentizer_via_dataset</span><span class="p">(</span><span class="n">input_path</span><span class="o">=</span><span class="s">"data/TinyStoriesV2-GPT4-train.txt"</span><span class="p">)</span>
</code></pre></div></div>

<p>And it will output the following info from the training process:</p>

<p>å®ƒå°†è¾“å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¥ä¸‹ä¿¡æ¯ï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>================================================================================
BPE TOKENIZER TRAINING ON TINYSTORIES DATASET
================================================================================
Configuration:
  Input file: /content/TinyStoriesV2-GPT4-train.txt
  File size: 2,227,753,162 bytes (2124.6 MB)
  Target vocabulary size: 10,000
  Special tokens: ['&lt;|endoftext|&gt;']
  Verbose logging: Enabled

Step 1: Setting up parallel processing...
Using 12 processes for parallel tokenization
Finding chunk boundaries aligned with special token: &lt;|endoftext|&gt;
Initial guess of the chunk boundaries: [0, 185646096, 371292192, 556938288, 742584384, 928230480, 1113876576, 1299522672, 1485168768, 1670814864, 1856460960, 2042107056, 2227753152]
Created 12 chunks for processing

Step 2: Pre-tokenizing text corpus...
Pre-tokenization completed in 66.52 seconds
Found 59,904 unique word types
Total token count: 536,592,162
Most common words:
  1. '.' -&gt; 41,764,519 times
  2. ',' -&gt; 23,284,331 times
  3. ' the' -&gt; 20,828,576 times
  4. ' and' -&gt; 19,475,966 times
  5. ' a' -&gt; 15,063,529 times

Step 3: Training BPE with 9,743 merges...
Initial vocabulary size: 257 (256 bytes + 1 special tokens)
============================================================
Merge    1/9743: ' ' + 't' -&gt; ' t' (freq: 63,482,199, time: 0.273s)
Merge    2/9743: 'h' + 'e' -&gt; 'he' (freq: 63,341,860, time: 0.318s)
Merge    3/9743: ' ' + 'a' -&gt; ' a' (freq: 47,465,635, time: 0.340s)
Merge    4/9743: ' ' + 's' -&gt; ' s' (freq: 32,362,158, time: 0.340s)
Merge    5/9743: ' ' + 'w' -&gt; ' w' (freq: 31,485,643, time: 0.327s)
Merge    6/9743: 'n' + 'd' -&gt; 'nd' (freq: 28,922,386, time: 0.332s)
Merge    7/9743: ' t' + 'he' -&gt; ' the' (freq: 28,915,024, time: 0.320s)
Merge    8/9743: 'e' + 'd' -&gt; 'ed' (freq: 24,836,456, time: 0.317s)
Merge    9/9743: ' ' + 'b' -&gt; ' b' (freq: 22,147,488, time: 0.326s)
Merge   10/9743: ' t' + 'o' -&gt; ' to' (freq: 20,892,273, time: 0.322s)
Merge  100/9743: ' ha' + 'pp' -&gt; ' happ' (freq: 3,147,884, time: 0.251s)
Merge  200/9743: ' s' + 'e' -&gt; ' se' (freq: 1,410,130, time: 0.343s)
Merge  300/9743: ' s' + 'omet' -&gt; ' somet' (freq: 790,510, time: 0.245s)
Merge  400/9743: ' g' + 'ot' -&gt; ' got' (freq: 524,776, time: 0.338s)
Merge  500/9743: ' e' + 'ach' -&gt; ' each' (freq: 369,637, time: 0.321s)
Merge  600/9743: 'l' + 'f' -&gt; 'lf' (freq: 279,566, time: 0.230s)
Merge  700/9743: ' wal' + 'k' -&gt; ' walk' (freq: 221,114, time: 0.237s)
Merge  800/9743: ' do' + 'll' -&gt; ' doll' (freq: 177,602, time: 0.324s)
Merge  900/9743: ' ' + 'G' -&gt; ' G' (freq: 147,699, time: 0.214s)
Merge 1000/9743: 'ec' + 't' -&gt; 'ect' (freq: 127,288, time: 0.233s)
Merge 1100/9743: ' l' + 'ight' -&gt; ' light' (freq: 108,006, time: 0.208s)
Merge 1200/9743: ' d' + 'in' -&gt; ' din' (freq: 92,211, time: 0.225s)
Merge 1300/9743: ' picture' + 's' -&gt; ' pictures' (freq: 80,416, time: 0.318s)
Merge 1400/9743: 'itt' + 'en' -&gt; 'itten' (freq: 68,466, time: 0.235s)
Merge 1500/9743: 'A' + 'my' -&gt; 'Amy' (freq: 59,829, time: 0.306s)
Merge 1600/9743: ' tal' + 'king' -&gt; ' talking' (freq: 53,781, time: 0.330s)
Merge 1700/9743: 'b' + 'all' -&gt; 'ball' (freq: 48,005, time: 0.309s)
Merge 1800/9743: ' k' + 'iss' -&gt; ' kiss' (freq: 43,477, time: 0.318s)
...
Merge 8000/9743: ' mom' + 'mies' -&gt; ' mommies' (freq: 879, time: 0.205s)
Merge 8100/9743: ' cryst' + 'als' -&gt; ' crystals' (freq: 840, time: 0.299s)
Merge 8200/9743: ' playd' + 'ate' -&gt; ' playdate' (freq: 809, time: 0.283s)
Merge 8300/9743: ' support' + 'ing' -&gt; ' supporting' (freq: 778, time: 0.200s)
Merge 8400/9743: ' activ' + 'ity' -&gt; ' activity' (freq: 747, time: 0.300s)
Merge 8500/9743: 'L' + 'izzy' -&gt; 'Lizzy' (freq: 716, time: 0.284s)
Merge 8600/9743: 'er' + 'ing' -&gt; 'ering' (freq: 691, time: 0.311s)
Merge 8700/9743: ' tid' + 'ied' -&gt; ' tidied' (freq: 660, time: 0.308s)
Merge 8800/9743: 'f' + 'lowers' -&gt; 'flowers' (freq: 633, time: 0.295s)
Merge 8900/9743: ' Gra' + 'nd' -&gt; ' Grand' (freq: 609, time: 0.299s)
Merge 9000/9743: ' frustr' + 'ation' -&gt; ' frustration' (freq: 584, time: 0.301s)
Merge 9100/9743: 'amil' + 'iar' -&gt; 'amiliar' (freq: 561, time: 0.205s)
Merge 9200/9743: ' P' + 'retty' -&gt; ' Pretty' (freq: 542, time: 0.310s)
Merge 9300/9743: ' sal' + 'on' -&gt; ' salon' (freq: 521, time: 0.292s)
Merge 9400/9743: ' p' + 'ounced' -&gt; ' pounced' (freq: 502, time: 0.196s)
Merge 9500/9743: ' pops' + 'ic' -&gt; ' popsic' (freq: 485, time: 0.185s)
Merge 9600/9743: ' pain' + 'ful' -&gt; ' painful' (freq: 469, time: 0.298s)
Merge 9700/9743: 'solut' + 'ely' -&gt; 'solutely' (freq: 454, time: 0.308s)
============================================================
BPE training completed in 2731.72 seconds
Final vocabulary size: 10000
Total merges performed: 9743
Compression ratio: 4.07x (from 2,192,422,648 to 538,511,097 tokens)

================================================================================
TRAINING SUMMARY
================================================================================
Total training time: 2898.45 seconds
Final vocabulary size: 10,000
Number of merges performed: 9,743
Actual vocab size vs target: 10000 / 10000

Saving tokenizer to disk...
  âœ“ Vocabulary saved to: tinystories_vocab.pkl
  âœ“ Merges saved to: tinystories_merges.pkl

================================================================================
VOCABULARY ANALYSIS
================================================================================
Token type breakdown:
  Byte tokens (0-255): 256
  Special tokens: 1
  Merged tokens: 9743
  Total: 10000

Byte tokens (first 10):
  Token   0: b'\x00' -&gt; '\x00'
  Token   1: b'\x01' -&gt; '\x01'
  Token   2: b'\x02' -&gt; '\x02'
  Token   3: b'\x03' -&gt; '\x03'
  Token   4: b'\x04' -&gt; '\x04'
  Token   5: b'\x05' -&gt; '\x05'
  Token   6: b'\x06' -&gt; '\x06'
  Token   7: b'\x07' -&gt; '\x07'
  Token   8: b'\x08' -&gt; '\x08'
  Token   9: b'\t' -&gt; '\t'

Special tokens:
  Token 256: b'&lt;|endoftext|&gt;' -&gt; '&lt;|endoftext|&gt;'

Most recently merged tokens (last 10):
  Token 9990: b' improving' -&gt; ' improving'
  Token 9991: b' nicest' -&gt; ' nicest'
  Token 9992: b' whiskers' -&gt; ' whiskers'
  Token 9993: b' booth' -&gt; ' booth'
  Token 9994: b' Land' -&gt; ' Land'
  Token 9995: b'Rocky' -&gt; 'Rocky'
  Token 9996: b' meadows' -&gt; ' meadows'
  Token 9997: b' Starry' -&gt; ' Starry'
  Token 9998: b' imaginary' -&gt; ' imaginary'
  Token 9999: b' bold' -&gt; ' bold'

First 10 merge operations:
  Merge  1: ' ' + 't' -&gt; ' t'
  Merge  2: 'h' + 'e' -&gt; 'he'
  Merge  3: ' ' + 'a' -&gt; ' a'
  Merge  4: ' ' + 's' -&gt; ' s'
  Merge  5: ' ' + 'w' -&gt; ' w'
  Merge  6: 'n' + 'd' -&gt; 'nd'
  Merge  7: ' t' + 'he' -&gt; ' the'
  Merge  8: 'e' + 'd' -&gt; 'ed'
  Merge  9: ' ' + 'b' -&gt; ' b'
  Merge 10: ' t' + 'o' -&gt; ' to'

Last 10 merge operations:
  Merge 9734: ' impro' + 'ving' -&gt; ' improving'
  Merge 9735: ' nice' + 'st' -&gt; ' nicest'
  Merge 9736: ' wh' + 'iskers' -&gt; ' whiskers'
  Merge 9737: ' bo' + 'oth' -&gt; ' booth'
  Merge 9738: ' L' + 'and' -&gt; ' Land'
  Merge 9739: 'Rock' + 'y' -&gt; 'Rocky'
  Merge 9740: ' meadow' + 's' -&gt; ' meadows'
  Merge 9741: ' St' + 'arry' -&gt; ' Starry'
  Merge 9742: ' imag' + 'inary' -&gt; ' imaginary'
  Merge 9743: ' bo' + 'ld' -&gt; ' bold'

Output file sizes:
  Vocabulary file: 117,701 bytes (114.9 KB)
  Merges file: 109,714 bytes (107.1 KB)
  Total: 227,415 bytes (222.1 KB)

================================================================================
TRAINING COMPLETED SUCCESSFULLY!
================================================================================
You can now use the trained tokenizer for encoding/decoding text.
Load with: vocab, merges = load_tokenizer('tinystories_vocab.pkl', 'tinystories_merges.pkl')
</code></pre></div></div>

<h2 id="using-the-trained-tokenizer">Using the Trained Tokenizer</h2>

<h2 id="ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†è¯å™¨">ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†è¯å™¨</h2>

<p>Once we have a trained tokenizer, we need a class to encode and decode text. Hereâ€™s one complete implementation:</p>

<p>ä¸€æ—¦æˆ‘ä»¬æœ‰äº†è®­ç»ƒå¥½çš„åˆ†è¯å™¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç±»æ¥ç¼–ç å’Œè§£ç æ–‡æœ¬ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å®ç°ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleBPETokenizer</span><span class="p">:</span>
    <span class="s">"""Simple BPE tokenizer for encoding/decoding text."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>  <span class="c1"># {token_id: bytes}
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="n">merges</span>  <span class="c1"># [(left_bytes, right_bytes), ...]
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="s">"&lt;|endoftext|&gt;"</span><span class="p">]</span>

        <span class="c1"># Create reverse mapping for decoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># GPT-2 style regex pattern
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s">"""'(?:[sdmt]|ll|ve|re)| ?[a-zA-ZÃ€-Ã¿]+| ?[0-9]+| ?[^\s\w]+|\s+(?!\S)|\s+"""</span>

        <span class="c1"># Build merge rules for faster encoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merges</span><span class="p">):</span>
            <span class="c1"># Find what tokens these bytes correspond to
</span>            <span class="n">left_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">left_bytes</span><span class="p">)</span>
            <span class="n">right_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">right_bytes</span><span class="p">)</span>
            <span class="n">merged_bytes</span> <span class="o">=</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span>
            <span class="n">merged_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">merged_bytes</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">left_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">right_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">merged_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[(</span><span class="n">left_id</span><span class="p">,</span> <span class="n">right_id</span><span class="p">)]</span> <span class="o">=</span> <span class="n">merged_id</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode text to token IDs."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="c1"># Handle special tokens
</span>        <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">remaining_text</span> <span class="o">=</span> <span class="n">text</span>

        <span class="c1"># Split on special tokens first
</span>        <span class="k">for</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">special_token</span> <span class="ow">in</span> <span class="n">remaining_text</span><span class="p">:</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">remaining_text</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">special_token</span><span class="p">)</span>
                <span class="n">new_parts</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">part</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parts</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Add special token
</span>                        <span class="n">special_bytes</span> <span class="o">=</span> <span class="n">special_token</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
                        <span class="n">special_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bytes_to_id</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">special_bytes</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">special_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                            <span class="n">token_ids</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">special_id</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">part</span><span class="p">:</span>
                        <span class="n">new_parts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
                <span class="n">remaining_text</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">new_parts</span><span class="p">)</span>

        <span class="c1"># Apply regex tokenization
</span>        <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="n">finditer</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pattern</span><span class="p">,</span> <span class="n">remaining_text</span><span class="p">):</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">match</span><span class="p">.</span><span class="n">group</span><span class="p">()</span>
            <span class="n">word_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_encode_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">token_ids</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>

    <span class="k">def</span> <span class="nf">_encode_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="s">"""Encode a single word using BPE merges."""</span>
        <span class="c1"># Start with individual bytes
</span>        <span class="n">word_bytes</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Convert each byte to its token ID
</span>        <span class="k">for</span> <span class="n">byte_val</span> <span class="ow">in</span> <span class="n">word_bytes</span><span class="p">:</span>
            <span class="n">tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">byte_val</span><span class="p">)</span>  <span class="c1"># Byte token IDs are 0-255
</span>
        <span class="c1"># Apply BPE merges
</span>        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Find the best merge to apply
</span>            <span class="n">best_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">best_pos</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">:</span>
                    <span class="c1"># Find merge priority (earlier merges have higher priority)
</span>                    <span class="n">merged_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">left_bytes</span><span class="p">,</span> <span class="n">right_bytes</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merges</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">left_bytes</span> <span class="o">+</span> <span class="n">right_bytes</span> <span class="o">==</span> <span class="n">merged_bytes</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">best_merge_priority</span><span class="p">:</span>
                                <span class="n">best_merge</span> <span class="o">=</span> <span class="n">pair</span>
                                <span class="n">best_pos</span> <span class="o">=</span> <span class="n">i</span>
                                <span class="n">best_merge_priority</span> <span class="o">=</span> <span class="n">j</span>
                            <span class="k">break</span>

            <span class="k">if</span> <span class="n">best_merge</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Apply the best merge
</span>            <span class="n">new_tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">best_pos</span><span class="p">]</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">merge_rules</span><span class="p">[</span><span class="n">best_merge</span><span class="p">])</span>
            <span class="n">new_tokens</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">best_pos</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:])</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">new_tokens</span>

        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Decode token IDs back to text."""</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">return</span> <span class="s">""</span>

        <span class="c1"># Convert token IDs to bytes
</span>        <span class="n">byte_sequences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">:</span>
                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Handle unknown tokens
</span>                <span class="n">byte_sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="sa">b</span><span class="s">'&lt;UNK&gt;'</span><span class="p">)</span>

        <span class="c1"># Concatenate all bytes and decode
</span>        <span class="n">all_bytes</span> <span class="o">=</span> <span class="sa">b</span><span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">byte_sequences</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize_with_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="s">"""Tokenize text and show detailed breakdown."""</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original text: '</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s"> characters"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"UTF-8 bytes: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span><span class="si">}</span><span class="s"> bytes"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token count: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">}</span><span class="s"> tokens"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Compression ratio: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">()</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"Token breakdown:"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
            <span class="n">token_bytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">id_to_bytes</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">token_str</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">token_str</span><span class="p">.</span><span class="n">isprintable</span><span class="p">():</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">token_str</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">token_str</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">. Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">:</span><span class="mi">4</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

        <span class="c1"># Verify round-trip
</span>        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Decoded text: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Round-trip successful: </span><span class="si">{</span><span class="n">text</span> <span class="o">==</span> <span class="n">decoded</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div>

<p>Let us compose some simple test cases below:</p>

<p>è®©æˆ‘ä»¬ç¼–å†™ä¸€äº›ç®€å•çš„æµ‹è¯•ç”¨ä¾‹ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_bpe_tokenizer</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"BPE TOKENIZER SAMPLE TESTS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>

    <span class="c1"># Load the trained tokenizer
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span> <span class="o">=</span> <span class="n">load_tokenizer</span><span class="p">(</span><span class="s">'tinystories_vocab.pkl'</span><span class="p">,</span> <span class="s">'tinystories_merges.pkl'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"âœ“ Loaded tokenizer with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s"> vocab entries and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="s"> merges"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Error: Tokenizer files not found!"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Please run the training script first to create 'tinystories_vocab.pkl' and 'tinystories_merges.pkl'"</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Create tokenizer instance
</span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SimpleBPETokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">merges</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 1: Simple sentence
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 1: Simple sentence"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="s">"Hello world! How are you today?"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 2: Text with special token
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 2: Text with special token"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="s">"Once upon a time&lt;|endoftext|&gt;The end."</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 3: Repeated words (should compress well)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 3: Repeated words"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text3</span> <span class="o">=</span> <span class="s">"the the the cat cat sat sat on on the the mat mat"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text3</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 4: Numbers and punctuation
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 4: Numbers and punctuation"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text4</span> <span class="o">=</span> <span class="s">"I have 123 apples, 456 oranges, and 789 bananas!"</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize_with_details</span><span class="p">(</span><span class="n">text4</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Example 5: Just encoding/decoding
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"EXAMPLE 5: Simple encode/decode"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">text5</span> <span class="o">=</span> <span class="s">"This is a test."</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text5</span><span class="p">)</span>
    <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Original: '</span><span class="si">{</span><span class="n">text5</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Decoded: '</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s">'"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Match: </span><span class="si">{</span><span class="n">text5</span> <span class="o">==</span> <span class="n">decoded_text</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># Show some vocabulary statistics
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"VOCABULARY STATISTICS"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">byte_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">special_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">token_bytes</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="sa">b</span><span class="s">'&lt;|'</span> <span class="ow">in</span> <span class="n">token_bytes</span><span class="p">)</span>
    <span class="n">merged_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="n">byte_tokens</span> <span class="o">-</span> <span class="n">special_tokens</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Byte tokens (0-255): </span><span class="si">{</span><span class="n">byte_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Special tokens: </span><span class="si">{</span><span class="n">special_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Merged tokens: </span><span class="si">{</span><span class="n">merged_tokens</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Total vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># Show some example merged tokens
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Sample merged tokens:"</span><span class="p">)</span>
    <span class="n">merged_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&gt;=</span> <span class="mi">257</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">merged_token_ids</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
        <span class="n">token_bytes</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">token_id</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">decoded</span> <span class="o">=</span> <span class="n">token_bytes</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s">'replace'</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: '</span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="s">' (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">token_bytes</span><span class="p">)</span><span class="si">}</span><span class="s"> bytes)"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Token </span><span class="si">{</span><span class="n">token_id</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">token_bytes</span><span class="si">}</span><span class="s"> (binary)"</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="s">"="</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"All examples completed successfully!"</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="bpe-tokenizer-sample-tests">BPE Tokenizer Sample Tests</h1>

<h1 id="bpeåˆ†è¯å™¨ç¤ºä¾‹æµ‹è¯•">BPEåˆ†è¯å™¨ç¤ºä¾‹æµ‹è¯•</h1>

<p>Now run our complete test suite:</p>

<p>ç°åœ¨è¿è¡Œæˆ‘ä»¬å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_bpe_tokenizer</span><span class="p">()</span>
</code></pre></div></div>

<p>Based on the training output from the TinyStories dataset, here are the testing results:</p>

<p>åŸºäºTinyStoriesæ•°æ®é›†çš„è®­ç»ƒè¾“å‡ºï¼Œä»¥ä¸‹æ˜¯æµ‹è¯•ç»“æœï¼š</p>

<p>âœ“ Loaded tokenizer with 10000 vocab entries and 9743 merges</p>

<p>âœ“ åŠ è½½äº†åŒ…å«10000ä¸ªè¯æ±‡æ¡ç›®å’Œ9743æ¬¡åˆå¹¶çš„åˆ†è¯å™¨</p>

<h2 id="example-1-simple-sentence">Example 1: Simple sentence</h2>

<h2 id="ç¤ºä¾‹1ç®€å•å¥å­">ç¤ºä¾‹1ï¼šç®€å•å¥å­</h2>

<p><strong>Original text:</strong> â€˜Hello world! How are you today?â€™
<strong>Length:</strong> 31 characters
<strong>UTF-8 bytes:</strong> 31 bytes
<strong>Token count:</strong> 8 tokens
<strong>Compression ratio:</strong> 3.88x</p>

<p><strong>åŸå§‹æ–‡æœ¬ï¼š</strong> â€˜Hello world! How are you today?â€™
<strong>é•¿åº¦ï¼š</strong> 31ä¸ªå­—ç¬¦
<strong>UTF-8å­—èŠ‚ï¼š</strong> 31å­—èŠ‚
<strong>è¯å…ƒæ•°é‡ï¼š</strong> 8ä¸ªè¯å…ƒ
<strong>å‹ç¼©æ¯”ï¼š</strong> 3.88å€</p>

<h3 id="token-breakdown">Token breakdown:</h3>

<h3 id="è¯å…ƒåˆ†è§£">è¯å…ƒåˆ†è§£ï¼š</h3>

<ul>
  <li>Token 1183: â€˜Helloâ€™ (5 bytes)</li>
  <li>Token 1569: â€˜ worldâ€™ (6 bytes)</li>
  <li>Token 33: â€˜!â€™ (1 bytes)</li>
  <li>Token 2683: â€˜ Howâ€™ (4 bytes)</li>
  <li>Token 483: â€˜ areâ€™ (4 bytes)</li>
  <li>Token 349: â€˜ youâ€™ (4 bytes)</li>
  <li>Token 1709: â€˜ todayâ€™ (6 bytes)</li>
  <li>
    <p>Token 63: â€˜?â€™ (1 bytes)</p>
  </li>
  <li>è¯å…ƒ 1183: â€˜Helloâ€™ (5å­—èŠ‚)</li>
  <li>è¯å…ƒ 1569: â€˜ worldâ€™ (6å­—èŠ‚)</li>
  <li>è¯å…ƒ 33: â€˜!â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 2683: â€˜ Howâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 483: â€˜ areâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 349: â€˜ youâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 1709: â€˜ todayâ€™ (6å­—èŠ‚)</li>
  <li>è¯å…ƒ 63: â€˜?â€™ (1å­—èŠ‚)</li>
</ul>

<p><strong>Decoded text:</strong> â€˜Hello world! How are you today?â€™
<strong>Round-trip successful:</strong> True</p>

<p><strong>è§£ç æ–‡æœ¬ï¼š</strong> â€˜Hello world! How are you today?â€™
<strong>å¾€è¿”æˆåŠŸï¼š</strong> True</p>

<h2 id="example-2-text-with-special-token">Example 2: Text with special token</h2>

<h2 id="ç¤ºä¾‹2åŒ…å«ç‰¹æ®Šæ ‡è®°çš„æ–‡æœ¬">ç¤ºä¾‹2ï¼šåŒ…å«ç‰¹æ®Šæ ‡è®°çš„æ–‡æœ¬</h2>

<p><strong>Original text:</strong> â€˜Once upon a time&lt;|endoftext|&gt;The end.â€™
<strong>Length:</strong> 37 characters
<strong>UTF-8 bytes:</strong> 37 bytes
<strong>Token count:</strong> 8 tokens
<strong>Compression ratio:</strong> 4.62x</p>

<p><strong>åŸå§‹æ–‡æœ¬ï¼š</strong> â€˜Once upon a time&lt;|endoftext|&gt;The end.â€™
<strong>é•¿åº¦ï¼š</strong> 37ä¸ªå­—ç¬¦
<strong>UTF-8å­—èŠ‚ï¼š</strong> 37å­—èŠ‚
<strong>è¯å…ƒæ•°é‡ï¼š</strong> 8ä¸ªè¯å…ƒ
<strong>å‹ç¼©æ¯”ï¼š</strong> 4.62å€</p>

<h3 id="token-breakdown-1">Token breakdown:</h3>

<h3 id="è¯å…ƒåˆ†è§£-1">è¯å…ƒåˆ†è§£ï¼š</h3>

<ul>
  <li>Token 256: â€˜<code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>â€™ (13 bytes)</li>
  <li>Token 430: â€˜Onceâ€™ (4 bytes)</li>
  <li>Token 439: â€˜ uponâ€™ (5 bytes)</li>
  <li>Token 259: â€˜ aâ€™ (2 bytes)</li>
  <li>Token 398: â€˜ timeâ€™ (5 bytes)</li>
  <li>Token 410: â€˜Theâ€™ (3 bytes)</li>
  <li>Token 870: â€˜ endâ€™ (4 bytes)</li>
  <li>
    <p>Token 46: â€˜.â€™ (1 bytes)</p>
  </li>
  <li>è¯å…ƒ 256: â€˜<code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>â€™ (13å­—èŠ‚)</li>
  <li>è¯å…ƒ 430: â€˜Onceâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 439: â€˜ uponâ€™ (5å­—èŠ‚)</li>
  <li>è¯å…ƒ 259: â€˜ aâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 398: â€˜ timeâ€™ (5å­—èŠ‚)</li>
  <li>è¯å…ƒ 410: â€˜Theâ€™ (3å­—èŠ‚)</li>
  <li>è¯å…ƒ 870: â€˜ endâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 46: â€˜.â€™ (1å­—èŠ‚)</li>
</ul>

<p><strong>Decoded text:</strong> â€˜&lt;|endoftext|&gt;Once upon a timeThe end.â€™
<strong>Round-trip successful:</strong> False</p>

<p><strong>è§£ç æ–‡æœ¬ï¼š</strong> â€˜&lt;|endoftext|&gt;Once upon a timeThe end.â€™
<strong>å¾€è¿”æˆåŠŸï¼š</strong> False</p>

<h2 id="example-3-repeated-words">Example 3: Repeated words</h2>

<h2 id="ç¤ºä¾‹3é‡å¤å•è¯">ç¤ºä¾‹3ï¼šé‡å¤å•è¯</h2>

<p><strong>Original text:</strong> â€˜the the the cat cat sat sat on on the the mat matâ€™
<strong>Length:</strong> 49 characters
<strong>UTF-8 bytes:</strong> 49 bytes
<strong>Token count:</strong> 13 tokens
<strong>Compression ratio:</strong> 3.77x</p>

<p><strong>åŸå§‹æ–‡æœ¬ï¼š</strong> â€˜the the the cat cat sat sat on on the the mat matâ€™
<strong>é•¿åº¦ï¼š</strong> 49ä¸ªå­—ç¬¦
<strong>UTF-8å­—èŠ‚ï¼š</strong> 49å­—èŠ‚
<strong>è¯å…ƒæ•°é‡ï¼š</strong> 13ä¸ªè¯å…ƒ
<strong>å‹ç¼©æ¯”ï¼š</strong> 3.77å€</p>

<h3 id="token-breakdown-2">Token breakdown:</h3>

<h3 id="è¯å…ƒåˆ†è§£-2">è¯å…ƒåˆ†è§£ï¼š</h3>

<ul>
  <li>Token 7199: â€˜theâ€™ (3 bytes)</li>
  <li>Token 263: â€˜ theâ€™ (4 bytes)</li>
  <li>Token 263: â€˜ theâ€™ (4 bytes)</li>
  <li>Token 459: â€˜ catâ€™ (4 bytes)</li>
  <li>Token 459: â€˜ catâ€™ (4 bytes)</li>
  <li>Token 1091: â€˜ satâ€™ (4 bytes)</li>
  <li>Token 1091: â€˜ satâ€™ (4 bytes)</li>
  <li>Token 354: â€˜ onâ€™ (3 bytes)</li>
  <li>Token 354: â€˜ onâ€™ (3 bytes)</li>
  <li>Token 263: â€˜ theâ€™ (4 bytes)</li>
  <li>Token 263: â€˜ theâ€™ (4 bytes)</li>
  <li>Token 1492: â€˜ matâ€™ (4 bytes)</li>
  <li>
    <p>Token 1492: â€˜ matâ€™ (4 bytes)</p>
  </li>
  <li>è¯å…ƒ 7199: â€˜theâ€™ (3å­—èŠ‚)</li>
  <li>è¯å…ƒ 263: â€˜ theâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 263: â€˜ theâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 459: â€˜ catâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 459: â€˜ catâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 1091: â€˜ satâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 1091: â€˜ satâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 354: â€˜ onâ€™ (3å­—èŠ‚)</li>
  <li>è¯å…ƒ 354: â€˜ onâ€™ (3å­—èŠ‚)</li>
  <li>è¯å…ƒ 263: â€˜ theâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 263: â€˜ theâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 1492: â€˜ matâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 1492: â€˜ matâ€™ (4å­—èŠ‚)</li>
</ul>

<p><strong>Decoded text:</strong> â€˜the the the cat cat sat sat on on the the mat matâ€™
<strong>Round-trip successful:</strong> True</p>

<p><strong>è§£ç æ–‡æœ¬ï¼š</strong> â€˜the the the cat cat sat sat on on the the mat matâ€™
<strong>å¾€è¿”æˆåŠŸï¼š</strong> True</p>

<h2 id="example-4-numbers-and-punctuation">Example 4: Numbers and punctuation</h2>

<h2 id="ç¤ºä¾‹4æ•°å­—å’Œæ ‡ç‚¹ç¬¦å·">ç¤ºä¾‹4ï¼šæ•°å­—å’Œæ ‡ç‚¹ç¬¦å·</h2>

<p><strong>Original text:</strong> â€˜I have 123 apples, 456 oranges, and 789 bananas!â€™
<strong>Length:</strong> 48 characters
<strong>UTF-8 bytes:</strong> 48 bytes
<strong>Token count:</strong> 19 tokens
<strong>Compression ratio:</strong> 2.53x</p>

<p><strong>åŸå§‹æ–‡æœ¬ï¼š</strong> â€˜I have 123 apples, 456 oranges, and 789 bananas!â€™
<strong>é•¿åº¦ï¼š</strong> 48ä¸ªå­—ç¬¦
<strong>UTF-8å­—èŠ‚ï¼š</strong> 48å­—èŠ‚
<strong>è¯å…ƒæ•°é‡ï¼š</strong> 19ä¸ªè¯å…ƒ
<strong>å‹ç¼©æ¯”ï¼š</strong> 2.53å€</p>

<h3 id="token-breakdown-3">Token breakdown:</h3>

<h3 id="è¯å…ƒåˆ†è§£-3">è¯å…ƒåˆ†è§£ï¼š</h3>

<ul>
  <li>Token 73: â€˜Iâ€™ (1 bytes)</li>
  <li>Token 499: â€˜ haveâ€™ (5 bytes)</li>
  <li>Token 6314: â€˜ 1â€™ (2 bytes)</li>
  <li>Token 50: â€˜2â€™ (1 bytes)</li>
  <li>Token 51: â€˜3â€™ (1 bytes)</li>
  <li>Token 1836: â€˜ applesâ€™ (7 bytes)</li>
  <li>Token 44: â€˜,â€™ (1 bytes)</li>
  <li>Token 9079: â€˜ 4â€™ (2 bytes)</li>
  <li>Token 53: â€˜5â€™ (1 bytes)</li>
  <li>Token 54: â€˜6â€™ (1 bytes)</li>
  <li>Token 5193: â€˜ orangesâ€™ (8 bytes)</li>
  <li>Token 44: â€˜,â€™ (1 bytes)</li>
  <li>Token 267: â€˜ andâ€™ (4 bytes)</li>
  <li>Token 32: â€˜ â€˜ (1 bytes)</li>
  <li>Token 55: â€˜7â€™ (1 bytes)</li>
  <li>Token 56: â€˜8â€™ (1 bytes)</li>
  <li>Token 57: â€˜9â€™ (1 bytes)</li>
  <li>Token 3898: â€˜ bananasâ€™ (8 bytes)</li>
  <li>
    <p>Token 33: â€˜!â€™ (1 bytes)</p>
  </li>
  <li>è¯å…ƒ 73: â€˜Iâ€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 499: â€˜ haveâ€™ (5å­—èŠ‚)</li>
  <li>è¯å…ƒ 6314: â€˜ 1â€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 50: â€˜2â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 51: â€˜3â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 1836: â€˜ applesâ€™ (7å­—èŠ‚)</li>
  <li>è¯å…ƒ 44: â€˜,â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 9079: â€˜ 4â€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 53: â€˜5â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 54: â€˜6â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 5193: â€˜ orangesâ€™ (8å­—èŠ‚)</li>
  <li>è¯å…ƒ 44: â€˜,â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 267: â€˜ andâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 32: â€˜ â€˜ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 55: â€˜7â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 56: â€˜8â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 57: â€˜9â€™ (1å­—èŠ‚)</li>
  <li>è¯å…ƒ 3898: â€˜ bananasâ€™ (8å­—èŠ‚)</li>
  <li>è¯å…ƒ 33: â€˜!â€™ (1å­—èŠ‚)</li>
</ul>

<p><strong>Decoded text:</strong> â€˜I have 123 apples, 456 oranges, and 789 bananas!â€™
<strong>Round-trip successful:</strong> True</p>

<p><strong>è§£ç æ–‡æœ¬ï¼š</strong> â€˜I have 123 apples, 456 oranges, and 789 bananas!â€™
<strong>å¾€è¿”æˆåŠŸï¼š</strong> True</p>

<h2 id="example-5-simple-encodedecode">Example 5: Simple encode/decode</h2>

<h2 id="ç¤ºä¾‹5ç®€å•ç¼–ç è§£ç ">ç¤ºä¾‹5ï¼šç®€å•ç¼–ç /è§£ç </h2>

<p><strong>Original:</strong> â€˜This is a test.â€™
<strong>Token IDs:</strong> [1531, 431, 259, 2569, 46]
<strong>Decoded:</strong> â€˜This is a test.â€™
<strong>Match:</strong> True</p>

<p><strong>åŸå§‹æ–‡æœ¬ï¼š</strong> â€˜This is a test.â€™
<strong>è¯å…ƒIDï¼š</strong> [1531, 431, 259, 2569, 46]
<strong>è§£ç æ–‡æœ¬ï¼š</strong> â€˜This is a test.â€™
<strong>åŒ¹é…ï¼š</strong> True</p>

<h2 id="vocabulary-statistics">Vocabulary Statistics</h2>

<h2 id="è¯æ±‡ç»Ÿè®¡">è¯æ±‡ç»Ÿè®¡</h2>

<p><strong>Byte tokens (0-255):</strong> 256
<strong>Special tokens:</strong> 1
<strong>Merged tokens:</strong> 9743
<strong>Total vocabulary:</strong> 10000</p>

<p><strong>å­—èŠ‚è¯å…ƒï¼ˆ0-255ï¼‰ï¼š</strong> 256
<strong>ç‰¹æ®Šè¯å…ƒï¼š</strong> 1
<strong>åˆå¹¶è¯å…ƒï¼š</strong> 9743
<strong>æ€»è¯æ±‡é‡ï¼š</strong> 10000</p>

<h3 id="sample-merged-tokens">Sample merged tokens:</h3>

<h3 id="åˆå¹¶è¯å…ƒç¤ºä¾‹">åˆå¹¶è¯å…ƒç¤ºä¾‹ï¼š</h3>

<ul>
  <li>Token 257: â€˜ tâ€™ (2 bytes)</li>
  <li>Token 258: â€˜heâ€™ (2 bytes)</li>
  <li>Token 259: â€˜ aâ€™ (2 bytes)</li>
  <li>Token 260: â€˜ sâ€™ (2 bytes)</li>
  <li>Token 261: â€˜ wâ€™ (2 bytes)</li>
  <li>Token 262: â€˜ndâ€™ (2 bytes)</li>
  <li>Token 263: â€˜ theâ€™ (4 bytes)</li>
  <li>Token 264: â€˜edâ€™ (2 bytes)</li>
  <li>Token 265: â€˜ bâ€™ (2 bytes)</li>
  <li>
    <p>Token 266: â€˜ toâ€™ (3 bytes)</p>
  </li>
  <li>è¯å…ƒ 257: â€˜ tâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 258: â€˜heâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 259: â€˜ aâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 260: â€˜ sâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 261: â€˜ wâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 262: â€˜ndâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 263: â€˜ theâ€™ (4å­—èŠ‚)</li>
  <li>è¯å…ƒ 264: â€˜edâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 265: â€˜ bâ€™ (2å­—èŠ‚)</li>
  <li>è¯å…ƒ 266: â€˜ toâ€™ (3å­—èŠ‚)</li>
</ul>

<hr />
<p>All examples completed successfully!</p>

<hr />
<p>æ‰€æœ‰ç¤ºä¾‹æˆåŠŸå®Œæˆï¼</p>

  </div><a class="u-url" href="/chinese-online-content-for-llm/cs336/cs336-note-train-bpe-tinystories-part2/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chinese-online-content-for-llm/"></data>

  <div class="wrapper">
    <!-- Support Section -->
    <div class="footer-support-section">
      <h3>â˜• æ”¯æŒæˆ‘çš„åˆ›ä½œ Support My Work</h3>
      <p style="margin-bottom: 1em; opacity: 0.9;">å¦‚æœè¿™äº›å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿è¯·æˆ‘å–æ¯å’–å•¡ï¼<br>If you find my content helpful, consider buying me a coffee!</p>
      <div class="support-options">
        <a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" class="support-btn buymeacoffee-btn">
          <span class="btn-icon">â˜•</span>
          <span class="btn-text">Buy Me a Coffee</span>
        </a>
      </div>
    </div>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chinese-online-content-for-llm/feed.xml">
            <svg class="svg-icon orange"><use xlink:href="/chinese-online-content-for-llm/assets/minima-social-icons.svg#rss"></use></svg><span>Subscribe</span>
          </a>
        </p>
        <p>A curated collection of Chinese online content designed for Large Language Model training, evaluation, and research. Exploring high-quality Chinese language resources for multilingual AI development.
</p>
      </div>
      <div class="footer-col">
        <p>Â© 2025 ğŸ’ å¤§æ¨¡å‹æˆ‘éƒ½çˆ±</p>
      </div>
    </div>
  </div>

  <!-- Theme toggle button -->
  <button id="theme-toggle" aria-label="Toggle dark mode">ğŸŒ™</button>
</footer>

<style>
.footer-support-section {
  text-align: center;
  padding: 2em 0;
  margin-bottom: 2em;
  border-bottom: 1px solid var(--border-color);
}

.footer-support-section h3 {
  margin: 0 0 0.5em 0;
  color: var(--text-color);
}

.support-options {
  display: flex;
  gap: 1em;
  justify-content: center;
  flex-wrap: wrap;
}

.support-btn {
  display: inline-flex;
  align-items: center;
  gap: 0.5em;
  padding: 0.75em 1.5em;
  border: none;
  border-radius: 8px;
  font-size: 1em;
  font-weight: 600;
  text-decoration: none;
  cursor: pointer;
  transition: all 0.3s ease;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.support-btn:hover {
  transform: translateY(-2px);
  box-shadow: 0 4px 12px rgba(0,0,0,0.15);
}

.buymeacoffee-btn {
  background: #FFDD00;
  color: #000;
}

.buymeacoffee-btn:hover {
  background: #FFE633;
}

.btn-icon {
  font-size: 1.2em;
}

[data-theme="dark"] .footer-support-section {
  border-bottom-color: #444;
}

[data-theme="dark"] .support-btn {
  box-shadow: 0 2px 8px rgba(0,0,0,0.3);
}

[data-theme="dark"] .support-btn:hover {
  box-shadow: 0 4px 12px rgba(0,0,0,0.4);
}
</style>
<!-- Floating Support Button -->
<a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" id="floating-support-btn" class="floating-support-btn" aria-label="Support this site">
  <span class="support-heart">â¤ï¸</span>
  <span class="support-text">Support</span>
</a>

<!-- Support Modal -->
<div id="support-modal" class="support-modal" onclick="if(event.target === this) this.style.display='none'">
  <div class="support-modal-content">
    <button class="modal-close" onclick="document.getElementById('support-modal').style.display='none'" aria-label="Close modal">&times;</button>

    <h2 style="text-align: center; margin-top: 0;">â˜• æ”¯æŒæˆ‘çš„åˆ›ä½œ</h2>
    <p style="text-align: center; color: #666; margin-bottom: 2em;">
      å¦‚æœè¿™äº›å†…å®¹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿æ”¯æŒæˆ‘ç»§ç»­åˆ›ä½œï¼<br>
      <small>If you find my content helpful, please consider supporting my work!</small>
    </p>

    <div class="support-methods">
      <!-- Buy Me a Coffee -->
      <div class="support-method">
        <div class="method-icon" style="background: #FFDD00;">â˜•</div>
        <h3>Buy Me a Coffee</h3>
        <p>One-time support with PayPal, Card, etc.</p>
        <a href="https://buymeacoffee.com/YOUR_USERNAME" target="_blank" rel="noopener" class="method-btn" style="background: #FFDD00; color: #000;">
          Support on Buy Me a Coffee
        </a>
      </div>

      <!-- Ko-fi -->
      <div class="support-method">
        <div class="method-icon" style="background: #13C3FF; color: white;">ğŸ’™</div>
        <h3>Ko-fi</h3>
        <p>Zero fees! Support with PayPal or Card</p>
        <a href="https://ko-fi.com/bearbearyu1223_go_irish" target="_blank" rel="noopener" class="method-btn" style="background: #13C3FF; color: white;">
          Support on Ko-fi
        </a>
      </div>

      <!-- GitHub Sponsors (Optional) -->
      <div class="support-method">
        <div class="method-icon" style="background: #24292e; color: white;">â­</div>
        <h3>GitHub Sponsors</h3>
        <p>Monthly sponsorship for tech creators</p>
        <a href="https://github.com/sponsors/YOUR_USERNAME" target="_blank" rel="noopener" class="method-btn" style="background: #24292e; color: white;">
          Sponsor on GitHub
        </a>
      </div>
    </div>

    <p style="text-align: center; margin-top: 2em; color: #888; font-size: 0.9em;">
      æ„Ÿè°¢ä½ çš„æ”¯æŒï¼ä½ çš„æ¯ä¸€ä»½å¿ƒæ„éƒ½æ˜¯æˆ‘æŒç»­åˆ›ä½œçš„åŠ¨åŠ› â¤ï¸<br>
      <small>Thank you for your support! Every contribution motivates me to create more content.</small>
    </p>
  </div>
</div>

<style>
/* Floating Support Button */
.floating-support-btn {
  position: fixed;
  bottom: 30px;
  right: 30px;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border: none;
  border-radius: 50px;
  padding: 12px 24px;
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  box-shadow: 0 4px 16px rgba(102, 126, 234, 0.4);
  z-index: 999;
  display: flex;
  align-items: center;
  gap: 8px;
  transition: all 0.3s ease;
  text-decoration: none;
}

.floating-support-btn:hover {
  transform: translateY(-3px);
  box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
  color: white;
  text-decoration: none;
}

.support-heart {
  font-size: 1.2em;
  animation: heartbeat 1.5s ease-in-out infinite;
}

@keyframes heartbeat {
  0%, 100% { transform: scale(1); }
  25% { transform: scale(1.1); }
  50% { transform: scale(1); }
}

/* Support Modal */
.support-modal {
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.7);
  z-index: 1000;
  justify-content: center;
  align-items: center;
  padding: 20px;
  overflow-y: auto;
}

.support-modal-content {
  background: var(--bg-color);
  border-radius: 16px;
  padding: 2em;
  max-width: 900px;
  width: 100%;
  position: relative;
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
  max-height: 90vh;
  overflow-y: auto;
}

.modal-close {
  position: absolute;
  top: 15px;
  right: 15px;
  background: none;
  border: none;
  font-size: 2em;
  color: var(--text-color);
  cursor: pointer;
  line-height: 1;
  padding: 0;
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  transition: background 0.3s ease;
}

.modal-close:hover {
  background: rgba(0, 0, 0, 0.1);
}

.support-methods {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 1.5em;
  margin: 2em 0;
}

.support-method {
  border: 2px solid var(--border-color);
  border-radius: 12px;
  padding: 1.5em;
  text-align: center;
  background: var(--bg-color);
  transition: all 0.3s ease;
}

.support-method:hover {
  transform: translateY(-5px);
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.1);
  border-color: #667eea;
}

.method-icon {
  width: 60px;
  height: 60px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 2em;
  margin: 0 auto 1em auto;
}

.support-method h3 {
  margin: 0.5em 0;
  color: var(--text-color);
}

.support-method p {
  color: #666;
  font-size: 0.9em;
  margin: 0.5em 0 1em 0;
  min-height: 2.5em;
}

.method-btn {
  display: inline-block;
  padding: 0.75em 1.5em;
  border-radius: 8px;
  text-decoration: none;
  font-weight: 600;
  transition: all 0.3s ease;
}

.method-btn:hover {
  transform: scale(1.05);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

/* Dark mode adjustments */
[data-theme="dark"] .modal-close:hover {
  background: rgba(255, 255, 255, 0.1);
}

[data-theme="dark"] .support-method {
  background: #1a1a1a;
}

[data-theme="dark"] .support-method p {
  color: #a0aec0;
}

/* Responsive design */
@media (max-width: 768px) {
  .floating-support-btn {
    bottom: 20px;
    right: 20px;
    padding: 10px 20px;
    font-size: 14px;
  }

  .support-modal-content {
    padding: 1.5em;
    margin: 10px;
  }

  .support-methods {
    grid-template-columns: 1fr;
  }
}

@media (max-width: 480px) {
  .floating-support-btn {
    bottom: 15px;
    right: 15px;
    padding: 8px 16px;
    font-size: 13px;
  }

  .support-text {
    display: none;
  }
}
</style>
</body>

</html>
